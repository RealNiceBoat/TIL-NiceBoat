{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ObjectDetection.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"a4VTBNRhrslY","colab_type":"code","colab":{}},"source":["import os\n","import PIL\n","import pickle\n","import numpy as np\n","from tqdm import tqdm\n","from math import log, exp\n","from random import shuffle\n","from skimage.transform import resize\n","from IPython.display import Image, display\n","from PIL import ImageEnhance, ImageFont, ImageDraw\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.python.keras.utils.data_utils import Sequence\n","\n","tf.keras.backend.clear_session()  # For easy reset of notebook state.\n","\n","input_shape = (224,224,3)\n","\n","# EDIT THESE TO SUIT YOUR FOLDER NAMES\n","base_folder = '##EDIT THIS TO YOUR BASE FOLDER##'\n","model_save_folder = os.path.join( base_folder, '##WHERE YOU WANT TO SAVE YOUR MODELS##' )\n","data_folder = os.path.join( base_folder, '##YOUR DATA FOLDER##' )\n","# These are how my pickled data chunks are stored. Change this when you stop using the VOC cat-dog dataset\n","data_split_template = '{}-voc-catdog-data-pil-set'.format( tuple(input_shape[:2]) )\n","data_split_template = data_split_template + '{}.p'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecztMvb6xZYL","colab_type":"code","colab":{}},"source":["'''\n","Augmentation methods. We need to implement our own augmentation because native support in keras does not change the bounding box \n","labels for us as the image is altered. We need to do it ourselves.\n","'''\n","# Helper method: Computes the boundary of the image that includes all bboxes\n","def compute_reasonable_boundary(labels):\n","  bounds = [ (x-w/2, x+w/2, y-h/2, y+h/2) for _,x,y,w,h in labels]\n","  xmin = min([bb[0] for bb in bounds])\n","  xmax = min([bb[1] for bb in bounds])\n","  ymin = min([bb[2] for bb in bounds])\n","  ymax = min([bb[3] for bb in bounds])\n","  return xmin, xmax, ymin, ymax\n","\n","def aug_horizontal_flip(img, labels):\n","  flipped_labels = []\n","  for c,x,y,w,h in labels:\n","    flipped_labels.append( (c,1-x,y,w,h) )\n","  return img.transpose(PIL.Image.FLIP_LEFT_RIGHT), np.array(flipped_labels)\n","\n","def aug_crop(img, labels):\n","  # Compute bounds such that no boxes are cut out\n","  xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n","  # Choose crop_xmin from [0, xmin]\n","  crop_xmin = max( np.random.uniform() * xmin, 0 )\n","  # Choose crop_xmax from [xmax, 1]\n","  crop_xmax = min( xmax + (np.random.uniform() * (1-xmax)), 1 )\n","  # Choose crop_ymin from [0, ymin]\n","  crop_ymin = max( np.random.uniform() * ymin, 0 )\n","  # Choose crop_ymax from [ymax, 1]\n","  crop_ymax = min( ymax + (np.random.uniform() * (1-ymax)), 1 )\n","  # Compute the \"new\" width and height of the cropped image\n","  crop_w = crop_xmax - crop_xmin\n","  crop_h = crop_ymax - crop_ymin\n","  cropped_labels = []\n","  for c,x,y,w,h in labels:\n","    c_x = (x - crop_xmin) / crop_w\n","    c_y = (y - crop_ymin) / crop_h\n","    c_w = w / crop_w\n","    c_h = h / crop_h\n","    cropped_labels.append( (c,c_x,c_y,c_w,c_h) )\n","\n","  W,H = img.size\n","  # Compute the pixel coordinates and perform the crop\n","  impix_xmin = int(W * crop_xmin)\n","  impix_xmax = int(W * crop_xmax)\n","  impix_ymin = int(H * crop_ymin)\n","  impix_ymax = int(H * crop_ymax)\n","  return img.crop( (impix_xmin, impix_ymin, impix_xmax, impix_ymax) ), np.array( cropped_labels )\n","\n","def aug_translate(img, labels):\n","  # Compute bounds such that no boxes are cut out\n","  xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n","  trans_range_x = [-xmin, 1 - xmax]\n","  tx = trans_range_x[0] + (np.random.uniform() * (trans_range_x[1] - trans_range_x[0]))\n","  trans_range_y = [-ymin, 1 - ymax]\n","  ty = trans_range_y[0] + (np.random.uniform() * (trans_range_y[1] - trans_range_y[0]))\n","\n","  trans_labels = []\n","  for c,x,y,w,h in labels:\n","    trans_labels.append( (c,x+tx,y+ty,w,h) )\n","\n","  W,H = img.size\n","  tx_pix = int(W * tx)\n","  ty_pix = int(H * ty)\n","  return img.rotate(0, translate=(tx_pix, ty_pix)), np.array( trans_labels )\n","\n","def aug_colorbalance(img, labels, color_factors=[0.2,2.0]):\n","  factor = color_factors[0] + np.random.uniform() * (color_factors[1] - color_factors[0])\n","  enhancer = ImageEnhance.Color(img)\n","  return enhancer.enhance(factor), labels\n","\n","def aug_contrast(img, labels, contrast_factors=[0.2,2.0]):\n","  factor = contrast_factors[0] + np.random.uniform() * (contrast_factors[1] - contrast_factors[0])\n","  enhancer = ImageEnhance.Contrast(img)\n","  return enhancer.enhance(factor), labels\n","\n","def aug_brightness(img, labels, brightness_factors=[0.2,2.0]):\n","  factor = brightness_factors[0] + np.random.uniform() * (brightness_factors[1] - brightness_factors[0])\n","  enhancer = ImageEnhance.Brightness(img)\n","  return enhancer.enhance(factor), labels\n","\n","def aug_sharpness(img, labels, sharpness_factors=[0.2,2.0]):\n","  factor = sharpness_factors[0] + np.random.uniform() * (sharpness_factors[1] - sharpness_factors[0])\n","  enhancer = ImageEnhance.Sharpness(img)\n","  return enhancer.enhance(factor), labels\n","\n","# Performs no augmentations and returns the original image and bbox. Used for the validation images.\n","def aug_identity(pil_img, label_arr):\n","  return np.array(pil_img), label_arr\n","\n","# This is the default augmentation scheme that we will use for each training image.\n","def aug_default(img, labels, p={'flip':0.5, 'crop':0.2, 'translate':0.2, 'color':0.2, 'contrast':0.2, 'brightness':0.2, 'sharpness':0.2}):\n","  if p['color'] > np.random.uniform():\n","    img, labels = aug_colorbalance(img, labels)\n","  if p['contrast'] > np.random.uniform():\n","    img, labels = aug_contrast(img, labels)\n","  if p['brightness'] > np.random.uniform():\n","    img, labels = aug_brightness(img, labels)\n","  if p['sharpness'] > np.random.uniform():\n","    img, labels = aug_sharpness(img, labels)\n","  if p['flip'] > np.random.uniform():\n","    img, labels = aug_horizontal_flip(img, labels)\n","  if p['crop'] > np.random.uniform():\n","    img, labels = aug_crop(img, labels)\n","  if p['translate'] > np.random.uniform():\n","    img, labels = aug_translate(img, labels)\n","  return np.array(img), labels\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aal0UzF9eupx","colab_type":"code","colab":{}},"source":["# Each y label has shape of (batch,i,j,7)\n","def custom_loss(ytrue, ypred):\n","  obj_loss_weight = 1.0\n","  cat_loss_weight = 1.0\n","  loc_loss_weight = 1.0\n","  # ytrue's first channel is objectness, and it signals where gradients should be considered.\n","  # So ypred should only take it's predictions seriously where ytrue has a positive, otherwise it should not learn from the negatives.\n","  objectness_loss = tf.keras.losses.BinaryCrossentropy()( ytrue[:,:,:,:1], ypred[:,:,:,:1] )\n","  ypred = tf.where( ytrue[:,:,:,:1] != 0, ypred, 0 )\n","  category_loss = tf.keras.losses.CategoricalCrossentropy() ( ytrue[:,:,:,1:3], ypred[:,:,:,1:3] )\n","  localisation_loss = tf.keras.losses.Huber() ( ytrue[:,:,:,3:], ypred[:,:,:,3:] )\n","  return obj_loss_weight*objectness_loss + cat_loss_weight*category_loss + loc_loss_weight*localisation_loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zG1a1ao1JzVw","colab_type":"code","colab":{}},"source":["def basic_detection_model( input_shape, model_name='basic_detection_model' ):\n","  inputs = keras.Input(shape=input_shape)\n","  x = layers.Conv2D(32, 3, padding='same')(inputs)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(64, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(64, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  block_1_output = layers.MaxPooling2D(2)(x) # 112\n","\n","  x = layers.Conv2D(64, 3, padding='same')(block_1_output)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(64, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.add([x, block_1_output])\n","  x = layers.Conv2D(128, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  block_2_output = layers.MaxPooling2D(2)(x) #56\n","\n","  x = layers.Conv2D(128, 3, padding='same')(block_2_output)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(128, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.add([x, block_2_output])\n","  x = layers.Conv2D(256, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  block_3_output = layers.MaxPooling2D(2)(x) #28\n","\n","  x = layers.Conv2D(256, 3, padding='same')(block_3_output)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(256, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.add([x, block_3_output])\n","  x = layers.Conv2D(512, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  block_4_output = layers.MaxPooling2D(2)(x) #14\n","\n","  x = layers.Conv2D(512, 3, padding='same')(block_4_output)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(512, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.add([x, block_4_output])\n","  x = layers.Conv2D(1024, 3, padding='same')(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  block_5_output = layers.MaxPooling2D(2)(x) #7\n","\n","  x = layers.Conv2D(512, 3, padding='same')(block_5_output)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(512, 3, padding='valid')(x) #5\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Conv2D(512, 3, padding='valid')(x) #3\n","  x = layers.BatchNormalization()(x)\n","  x = layers.ReLU()(x)\n","\n","  x = layers.Dropout(0.5)(x)\n","\n","  objectness_preds = layers.Conv2D(1, 1, activation='sigmoid')(x)\n","  class_preds = layers.Conv2D(2, 1, activation='softmax')(x)\n","  bbox_preds = layers.Conv2D(4, 1)(x)\n","  predictions = layers.Concatenate()( [objectness_preds, class_preds, bbox_preds] ) # result is (3,3,7)\n","\n","  model = keras.Model(inputs, predictions, name=model_name)\n","  model.compile( optimizer=tf.keras.optimizers.Adam(0.001),\n","                 loss=custom_loss )\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"as7KeMy-WDs0","colab_type":"code","colab":{}},"source":["# Choose whether to start a new model or load a previously trained one\n","model_context = 'object-detection-tutorial'\n","# load_model_path = os.path.join( base_folder, model_save_folder, '{}-best_val_loss.h5'.format(model_context) )\n","load_model_path = None\n","if load_model_path is not None:\n","  model = tf.keras.models.load_model( load_model_path , custom_objects={'custom_loss':custom_loss})\n","else:\n","  model = basic_detection_model(input_shape=input_shape, model_name=model_context)\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RzuhzYLEH5jA","colab_type":"code","colab":{}},"source":["def reload_data(set_indices, num_sets=None):\n","  if num_sets is not None:\n","    shuffle( set_indices )\n","  selected_indices = set_indices[:num_sets]\n","  acc = []\n","  for index in selected_indices:\n","    set_fp = os.path.join( data_folder, data_split_template.format(index) )\n","    with open(set_fp, 'rb') as f:\n","      mini_dataset = pickle.load(f)\n","      acc.extend(mini_dataset)\n","  shuffle(acc)\n","  return acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYxBEj-kY9HH","colab_type":"code","colab":{}},"source":["  # Computes the intersection-over-union (IoU) of two bounding boxes\n","  def iou(bb1, bb2):\n","    x1,y1,w1,h1 = bb1\n","    xmin1 = x1 - w1/2\n","    xmax1 = x1 + w1/2\n","    ymin1 = y1 - h1/2\n","    ymax1 = y1 + h1/2\n","\n","    x2,y2,w2,h2 = bb2\n","    xmin2 = x2 - w2/2\n","    xmax2 = x2 + w2/2\n","    ymin2 = y2 - h2/2\n","    ymax2 = y2 + h2/2\n","\n","    area1 = w1*h1\n","    area2 = w2*h2\n","\n","    # Compute the boundary of the intersection\n","    xmin_int = max( xmin1, xmin2 )\n","    xmax_int = min( xmax1, xmax2 )\n","    ymin_int = max( ymin1, ymin2 )\n","    ymax_int = min( ymax1, ymax2 )\n","    intersection = max(xmax_int - xmin_int, 0) * max( ymax_int - ymin_int, 0 )\n","\n","    # Remove the double counted region\n","    union = area1+area2-intersection\n","\n","    return intersection / union"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcPEGkvKjkBB","colab_type":"code","colab":{}},"source":["class CatDogVocSequence(Sequence):\n","  def __init__(self, dataset, batch_size, augmentations, dims, input_size=(224,224,3), iou_fn=iou):\n","    self.x, self.y = zip(*dataset)\n","    self.x_acc, self.y_acc = [], []\n","    self.batch_size = batch_size\n","    self.augment = augmentations\n","    self.dims = dims\n","    self.input_size = input_size\n","    self.iou = iou_fn\n","\n","  def __len__(self):\n","    return int(np.ceil(len(self.x) / float(self.batch_size)))\n","\n","  '''\n","  labels: A numpy array of shape (num_labels, 5). num_labels is the number of bounding boxes for the image.\n","  Each bounding box has entry: c x y w h (class, center-x, center-y, width, height). \n","  \n","  All numbers are normalized wrt image size: they are in the range [0,1]\n","\n","  This function inspects each bbox entry and decides how to generate a corresponding array format that the CNN understands.\n","  '''\n","  def convert_labels_cxywh_to_arrays(self, labels, iou_threshold=0.5, exceed_thresh_positive=True):\n","    num_entries = 7 # objectness, p_cat, p_dog, dx, dy, dw, dh\n","    kx,ky = self.dims\n","    labels_arr = np.zeros( (kx, ky, num_entries) ) # For this basic model, this is of shape (3,3,7)\n","\n","    for label in labels:\n","      # Retrieve the ground-truth class label and bbox\n","      gtclass, gtx, gty, gtw, gth = label\n","      gtclass = int(gtclass)\n","      gt_bbox = [gtx, gty, gtw, gth]\n","      \n","      iou_scores = []\n","\n","      '''\n","      There are kx x ky cells. In the basic model, this is 3x3.\n","      Each cell is of width=gapx and height=gapy\n","      For the (i,j)-th tile, center-x = (0.5+i)*gapx | center-y = (0.5+j)*gapy\n","      '''\n","      gapx = 1.0 / kx\n","      gapy = 1.0 / ky\n","      # In this loop, we run through all cells of the 3x3 grid, compute the intersection-over-union w the ground-truth and also the targets to predict.\n","      for i in range(kx):\n","        for j in range(ky):\n","          x = (0.5+i)*gapx\n","          y = (0.5+j)*gapy\n","\n","          # These are fixed to the width and height of the square-cell at the moment. However, if we want more anchor boxes of varying aspect ratios, this is the place to change it.\n","          w = gapx\n","          h = gapy\n","          candidate_bbox = [x,y,w,h]\n","\n","          # Based on the SSD training regime. These are the targets we wish for the CNN to predict at the end.\n","          # Read the SSD paper: https://arxiv.org/pdf/1512.02325.pdf, for more details.\n","          dx = (gtx - x) / w \n","          dy = (gty - y) / h\n","          dw = log( gtw / w )\n","          dh = log( gth / h )\n","\n","          IoU = self.iou( candidate_bbox, gt_bbox )\n","          iou_scores.append( (IoU, i, j, dx, dy, dw, dh) )\n","      # Sort by IoU: only the highest IoU scores get included into the resulting label array. Cutoff at threshold.\n","      iou_scores.sort( key=lambda x: x[0], reverse=True )\n","      # Count the top 25% of iou scores\n","      top_count = max( round(len(iou_scores) * 0.25), 1)\n","      # Remove all the grid cells that do not overlap with ground truth at all\n","      iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] > 0]\n","      iou_scores = iou_scores[:top_count] + [iou_score for iou_score in iou_scores[top_count:] if iou_score[0] >= iou_threshold]\n","      # Always take the top IoU entry\n","      for iou_score in iou_scores:\n","        # The top IoU score is always included\n","        IoU, i, j, dx, dy, dw, dh = iou_score\n","        payload = [IoU, 0, 0, dx,dy,dw,dh]\n","        payload[gtclass + 1] = 1\n","        labels_arr[i,j,:] = payload\n","    return labels_arr\n","\n","  # Basic preprocessing that can be replaced, if you want to try\n","  def preprocess_npimg(self, x):\n","    return x * 1./255.\n","\n","  def __getitem__(self, idx):\n","    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n","    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","    self.x_acc.clear()\n","    self.y_acc.clear()\n","    for x,y in zip( batch_x, batch_y ):\n","      x_aug, y_aug = self.augment( x, y )\n","      self.x_acc.append( x_aug if x_aug.shape == self.input_size else resize( x_aug, self.input_size[:2] ) )\n","      y_arr = self.convert_labels_cxywh_to_arrays( y_aug )\n","      self.y_acc.append( y_arr )\n","\n","    return self.preprocess_npimg( np.array( self.x_acc ) ), np.array( self.y_acc )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JCGXYh7vY3KV","colab_type":"code","colab":{}},"source":["# I broke up my dataset into 10 pickle files because I had issues at the start getting all of them into one giant set. \n","# Everything seems okay now, but I have kept this format.\n","# You can set up your own way of reading in the data.\n","all_set_ids = list(range(10))\n","val_set_ids = [0]\n","train_set_ids = all_set_ids[len(val_set_ids):]\n","\n","dims = (3,3)\n","bs = 32\n","n_epochs = 100\n","\n","train_dataset = reload_data(train_set_ids)\n","# Set up training sequence data generator with default augmentation\n","train_sequence = CatDogVocSequence(train_dataset, bs, aug_default, dims)\n","val_dataset = reload_data(val_set_ids)\n","# Set up validation sequence data generator with no augmentation\n","val_sequence = CatDogVocSequence(val_dataset, bs, aug_identity, dims)\n","\n","# 3 checkpoints in use - one to save the best val_loss model, one to stop early if no improvement, and one to reduce the learning rate if no improvement.\n","save_model_path = os.path.join( base_folder, model_save_folder, '{}-best_val_loss.h5'.format(model_context) )\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=save_model_path,\n","    save_weights_only=False,\n","    monitor='val_loss',\n","    mode='auto',\n","    save_best_only=True)\n","earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n","\n","model.fit(x=train_sequence, \n","          epochs=n_epochs, \n","          batch_size=bs, \n","          validation_data=val_sequence, \n","          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr],\n","          )\n","\n","# Save the final one, if you want\n","model.save(os.path.join(base_folder, model_save_folder, '{}-final.h5'.format(model_context)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctFEOcg7caCo","colab_type":"code","colab":{}},"source":["# Load a previously saved model\n","model_context = 'object-detection-tutorial'\n","saved_model_path = os.path.join( base_folder, model_save_folder, '{}-best_val_loss.h5'.format(model_context) )\n","# saved_model_path = os.path.join( base_folder, model_save_folder, '{}-final.h5'.format(model_context) )\n","model = tf.keras.models.load_model(saved_model_path, custom_objects={'custom_loss':custom_loss})\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kkDh5lRDuYy7","colab_type":"code","colab":{}},"source":["# Load some data to use on the trained model.\n","dataset = reload_data([0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDts8dfzd-Gj","colab_type":"code","colab":{}},"source":["# Converts model tensor outputs back into c,x,y,w,h format so that we can display results.\n","def convert_array_to_cxywh(label_arr, det_threshold=0.1, top=None):\n","  kx, ky = label_arr.shape[:2]\n","  gapx = 1. / kx\n","  gapy = 1. / ky\n","  # Find the locations of the label_arr where the objectness-score (detection confidence) exceeds the threshold. \n","  # These are the detections we will visualize.\n","  # The lower the threshold, the more false positives we are likely to get.\n","  eyes, jays = np.where( label_arr[:,:,0] > det_threshold ) #i's and j's are the coordinates of the tensor.\n","  labels = []\n","  for i,j in zip(eyes,jays):\n","    cx = (0.5+i)*gapx\n","    cy = (0.5+j)*gapy\n","    w = gapx\n","    h = gapy\n","\n","    det_score, p_cat, p_dog, dx, dy, dw, dh = label_arr[i,j]\n","\n","    # Reverse the targets based on the SSD formulation, to obtain proper x,y,w,h information\n","    predx = (dx * w) + cx\n","    predy = (dy * h) + cy\n","    predw = w * exp( dw )\n","    predh = h * exp( dh )\n","    class_str = 'cat' if p_cat > p_dog else 'dog'\n","    labels.append( (det_score, class_str, predx, predy, predw, predh, i, j) )\n","  labels.sort( key=lambda x:x[0], reverse=True )\n","  labels = labels[:top]\n","  return labels\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UT3xxxO8qjcr","colab_type":"code","colab":{}},"source":["# To fix multiple, we introduce non-maximum suppression, or NMS for short\n","def nms(detections, iou_thresh=0.):\n","  dets_by_class = {}\n","  final_result = []\n","  for det in detections:\n","    cls = det[1]\n","    if cls not in dets_by_class:\n","      dets_by_class[cls] = []\n","    dets_by_class[cls].append( det )\n","  for _, dets in dets_by_class.items():\n","    candidates = list(dets)\n","    candidates.sort( key=lambda x:x[0], reverse=True )\n","    while len(candidates) > 0:\n","      candidate = candidates.pop(0)\n","      _,_,cx,cy,cw,ch,_,_ = candidate\n","      copy = list(candidates)\n","      for other in candidates:\n","        # Compute the IoU. If it exceeds thresh, we remove it\n","        _,_,ox,oy,ow,oh,_,_ = other\n","        if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n","          copy.remove(other)\n","      candidates = list(copy)\n","      final_result.append(candidate)\n","  return final_result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPCo1rzPhfAd","colab_type":"code","colab":{}},"source":["# This snippet visualises the result.\n","colors = ['green', 'blue', 'yellow', 'red', 'cyan', 'magenta', 'white', 'orange', 'brown']\n","for k in range(0,10):\n","  pil_img, _ = dataset[k]\n","  img_arr = np.array(pil_img) / 255.\n","  W,H = pil_img.size\n","  model_pred = model(np.array([img_arr]))[0]\n","  preds = convert_array_to_cxywh( model_pred, det_threshold=0.1 )\n","\n","  preds = nms(preds, iou_thresh=0.1)\n","\n","  draw_img = pil_img.copy()\n","  draw = ImageDraw.Draw(draw_img)\n","  for _, cls,x,y,w,h,i,j in preds:\n","    bb_x = int(x * W)\n","    bb_y = int(y * H)\n","    bb_w = int(w * W)\n","    bb_h = int(h * H)\n","    left = int(bb_x - bb_w / 2)\n","    top = int(bb_y - bb_h / 2)\n","    right = int(bb_x + bb_w / 2)\n","    bot = int(bb_y + bb_h / 2)\n","    color = colors[i*3 + j]\n","\n","    draw.rectangle(((left, top), (right, bot)), outline=color)\n","    draw.text((bb_x, bb_y), cls, fill=color)\n","\n","  display(draw_img)\n"],"execution_count":0,"outputs":[]}]}