{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j06pz3I2MGws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This notebook covers how to use tf.keras to build a classification model like what we talked about in the previous series.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbcPzAQAwIIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "\n",
        "# CHANGE THESE TO FIT YOUR FOLDER NAMES\n",
        "base_folder = '##PUT YOUR BASE FOLDER HERE##'\n",
        "data_folder = os.path.join( base_folder, '##YOUR CATS AND DOGS FOLDER##' )\n",
        "save_folder = os.path.join( base_folder, '##WHERE YOU WANT TO SAVE YOUR MODELS##' )\n",
        "input_shape = (224,224,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWGL20iRqm52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvhoMX949-WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_classification_model(input_shape, model_name='basic_model'):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  x = layers.Conv2D(32, 3, padding='same')(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.Conv2D(64, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.Conv2D(64, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  block_1_output = layers.MaxPooling2D(2)(x) # 112\n",
        "\n",
        "  x = layers.Conv2D(64, 3, padding='same')(block_1_output)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.Conv2D(64, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.add([x, block_1_output])\n",
        "  x = layers.Conv2D(128, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  block_2_output = layers.MaxPooling2D(2)(x) #56\n",
        "\n",
        "  x = layers.Conv2D(128, 3, padding='same')(block_2_output)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.Conv2D(128, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.add([x, block_2_output])\n",
        "  x = layers.Conv2D(256, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  block_3_output = layers.MaxPooling2D(2)(x) #28\n",
        "\n",
        "  x = layers.Conv2D(256, 3, padding='same')(block_3_output)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.Conv2D(256, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.add([x, block_3_output])\n",
        "  x = layers.Conv2D(512, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  block_4_output = layers.MaxPooling2D(2)(x) #14\n",
        "\n",
        "  x = layers.Conv2D(512, 3, padding='same')(block_4_output)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.Conv2D(512, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.add([x, block_4_output])\n",
        "  x = layers.Conv2D(1024, 3, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  x = layers.MaxPooling2D(2)(x) #7\n",
        "\n",
        "  x = layers.GlobalAveragePooling2D()(x) #1024\n",
        "\n",
        "  x = layers.Dense(1024)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "\n",
        "  x = layers.Dropout(0.5)(x) # Alternatively, we can use:\n",
        "  x = layers.Dense(2)(x) # x = layers.Dense(1)(x)\n",
        "  predictions = layers.Softmax()(x) # predictions = layers.Sigmoid()(x)\n",
        "\n",
        "  model = keras.Model(inputs, predictions, name=model_name)\n",
        "  model.compile( optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                 loss=keras.losses.CategoricalCrossentropy(from_logits=False), # BinaryCrossentropy instead\n",
        "                 metrics=['accuracy'] )\n",
        "  return model\n",
        "  \n",
        "  # If you want to know more about optimizers: https://ruder.io/optimizing-gradient-descent/index.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tcEIvrxKM75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_context = 'basic_model'\n",
        "model = basic_classification_model(input_shape, model_name=model_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgkEXhw9LeKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To see what you've built, you can use model.summary()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-isWkF0KSkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can also view a graphical plot of the model\n",
        "model_plot = tf.keras.utils.plot_model(model, show_shapes=True)\n",
        "display(model_plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT1Q3hl9LgBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To train the model, we first need data. \n",
        "# Inside data_folder there are two folders: \"dog\" and \"cat\", which have images of dogs and cats respectively.\n",
        "# We will use keras's built-in utilities to read the images from these folders and feed them into our model for training.\n",
        "\n",
        "# But first, let's look at some of the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B80-R7zTO6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_folder = os.path.join( data_folder, 'cat' )\n",
        "cat_files = list(os.listdir( cat_folder ))\n",
        "dog_folder = os.path.join( data_folder, 'dog' )\n",
        "dog_files = list(os.listdir( dog_folder ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNWUjlctUGjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "# Look at the first cat image\n",
        "Image(filename= os.path.join( cat_folder, cat_files[0] ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSELoDidUJMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the last dog image\n",
        "Image(filename= os.path.join( dog_folder, dog_files[-1] ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABAK24COUd1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the data generators to read from our data_folder\n",
        "bs = 32 # The batch size is 32\n",
        "\n",
        "# An object that applies transformations to the images before they are consumed by the model\n",
        "# These transformations include (1) preprocessing, like rescaling or normalization (2) data augmentation\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255, # divide each pixel value by 255. Each pixel is in the range 0-255, so after division it is in 0-1\n",
        "        rotation_range=20, # rotate the image between -20 to +20 degrees\n",
        "        width_shift_range=0.2, # translate the image left-right for 20% of the image's width\n",
        "        height_shift_range=0.2, # same, for up-down and height\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2)\n",
        "print('Making training data generator...')\n",
        "train_gen = datagen.flow_from_directory(\n",
        "        data_folder,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=bs,\n",
        "        subset='training')\n",
        "print('Making validation data generator...')\n",
        "val_gen = datagen.flow_from_directory(\n",
        "        data_folder,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=bs,\n",
        "        subset='validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjZSyYr-cWH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callbacks are useful to help you monitor the progress of the training as it is going on\n",
        "# and to intervene in between if certain conditions are met.\n",
        "\n",
        "# This monitors the validation loss of the model as it is training, saving a full copy of the model and it's specs everytime the\n",
        "# validation loss reaches an unprecedented low.\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join( save_folder, '{}-best_val_loss.h5'.format(model_context) ),\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='auto',\n",
        "    save_best_only=True)\n",
        "\n",
        "# If the validation loss doesn't improve for 20 epochs, stop training\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "# If the validation loss doesn't improve for 5 epochs, reduce the learning rate to 0.2 times it's previous value\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Tj_m1baFeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start training the model\n",
        "n_epochs=50\n",
        "model.fit(train_gen,\n",
        "          epochs=n_epochs,\n",
        "          steps_per_epoch=train_gen.n // bs,\n",
        "          validation_data=val_gen,\n",
        "          validation_steps=val_gen.n // bs,\n",
        "          callbacks=[model_checkpoint, earlystopping, reduce_lr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fejGP2YqfCPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's load an independent image to try our model on\n",
        "# CHANGE TO YOUR FOLDER AND IMAGE NAMES\n",
        "test_img_path = os.path.join(base_folder, 'src_images', 'boss.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNDYODixV0JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def run_image_on_model(img_path, model, label_map):\n",
        "  pil_img = load_img(test_img_path)\n",
        "  pil_img = pil_img.resize( input_shape[:2] )\n",
        "  img_arr = img_to_array(pil_img)\n",
        "  # Remember to normalize the image values the same way you did when you trained the model\n",
        "  img_arr = img_arr / 255.\n",
        "  # We need to wrap this in an np.array with dimensions (b,H,W,C). Currently, the shape is only (H,W,C)\n",
        "  img_arr = np.array( [img_arr] )\n",
        "  pred = model.predict(img_arr, batch_size=1)[0]\n",
        "  pred_idx = np.argmax(pred)\n",
        "  return label_map[pred_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbsCNrmSaSXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The generator's internal labeling of cat/dog\n",
        "print(train_gen.class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcEmV7yMeEIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct a reverse mapping\n",
        "label_map = {v:k for k,v in train_gen.class_indices.items()}\n",
        "label_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETDFL6lXeS7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image(filename=test_img_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXlT2_aFe910",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your results may vary here. It's possible that your model will predict correctly\n",
        "print( 'model prediction: {}'.format(run_image_on_model(test_img_path, model, label_map)) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-IWlqvwhz1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try and improve the accuracy of the model by applying transfer learning.\n",
        "# The key idea behind transfer learning is to leverage on a more powerful model trained on a different but related task.\n",
        "# In this case, we are going to use a pre-trained ResNet50 from the keras applications module, that was pre-trained on a 1000-category classification dataset\n",
        "# Both are classification tasks, so they are related. However, the pre-trained model predicts a probability vector between 1000 classes\n",
        "# So we'd need to modify it to only predict cat and dog classes.\n",
        "\n",
        "# The features extracted by the powerful model are better than what we have now. The idea is to use these features to \n",
        "# \"warm-up\" our own model.\n",
        "# Once the model is sufficiently warmed up, we can train both parts together."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqja549cjgOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# When include_top=False, we are discarding the 1000 category predictions\n",
        "transfer_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoiDqvxIkEGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transfer_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhXkymr7kftG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Notice that we are left with a 7x7 square of depth 2048.\n",
        "# We will apply GAP to reduce this tensor to a vector of length 2048, and train a classifier at the end to distinguish between two classes\n",
        "# But first, we should disable training for the ResNet50 temporarily:\n",
        "for layer in transfer_model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fQGUzZHlKP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect the model to see that the number of trainable params is zero\n",
        "transfer_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFOu8EqylZMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's rig together a new model\n",
        "def transfer_learning_model(input_shape, base_model, model_name='transfer_learning_model'):\n",
        "  # Freeze the base model\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  # First, run the input through the power model. x contains good extracted features.\n",
        "  x = base_model(inputs)\n",
        "  # Notice that the rest below are more or less the same\n",
        "  x = layers.GlobalAveragePooling2D()(x) #2048\n",
        "\n",
        "  x = layers.Dense(1024)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.ReLU()(x)\n",
        "  \n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Dense(2)(x)\n",
        "  predictions = layers.Softmax()(x) # predictions = layers.Sigmoid()(x)\n",
        "\n",
        "  model = keras.Model(inputs, predictions, name=model_name)\n",
        "  # Fine tuning requires a lower learning rate. The pre-trained model will be upset by the new rookie layers otherwise.\n",
        "  model.compile( optimizer=tf.keras.optimizers.Adam(0.00001),\n",
        "                 loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "                 metrics=['accuracy'] )\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnHyoHj8mYiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transfer_learning_model = transfer_learning_model(input_shape, transfer_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3F6aiXmiEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check that the output is as expected\n",
        "transfer_learning_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLMi72Pzm6ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the same callbacks, but with a different model_context\n",
        "model_context = 'transfer_learning'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join( save_folder, '{}-best_val_loss.h5'.format(model_context) ),\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='auto',\n",
        "    save_best_only=True)\n",
        "\n",
        "# If the validation loss doesn't improve for 20 epochs, stop training\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "# If the validation loss doesn't improve for 5 epochs, reduce the learning rate to 0.2 times it's previous value\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsuMzHhmnEl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Warm up for 10 epochs\n",
        "n_epochs_warmup=10\n",
        "# Followed by 40 epochs with all params trainable\n",
        "n_epochs_fullblast=40\n",
        "\n",
        "print('Warming up for {} epochs...'.format(n_epochs_warmup))\n",
        "transfer_learning_model.fit(train_gen,\n",
        "          epochs=n_epochs_warmup,\n",
        "          steps_per_epoch=train_gen.n // bs,\n",
        "          validation_data=val_gen,\n",
        "          validation_steps=val_gen.n // bs,\n",
        "          callbacks=[model_checkpoint, earlystopping, reduce_lr])\n",
        "\n",
        "\n",
        "print('Done. Unfreezing all layers and training for {} more epochs...'.format(n_epochs_fullblast))\n",
        "# After the warm-up, unfreeze all the layers of the base ResNet50\n",
        "for layer in transfer_model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "transfer_learning_model.fit(train_gen,\n",
        "          epochs=n_epochs_fullblast,\n",
        "          steps_per_epoch=train_gen.n // bs,\n",
        "          validation_data=val_gen,\n",
        "          validation_steps=val_gen.n // bs,\n",
        "          callbacks=[model_checkpoint, earlystopping, reduce_lr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRz_5Gw5qYd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try again with the new transfer-learned model\n",
        "Image(filename=test_img_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NUOLrooCTVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print( 'model prediction: {}'.format(run_image_on_model(test_img_path, transfer_learning_model, label_map)) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmilJUUL8rN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary\n",
        "# Transfer learning and fine-tuning are good options to explore if you want to improve your model's performance. You need to\n",
        "# be mentally prepared to carefully tune learning rates and warm-up procedures though."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}