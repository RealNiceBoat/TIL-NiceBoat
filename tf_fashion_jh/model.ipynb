{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "TF Version:2.2.0  |  GPU:PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "device = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(device, True)\n",
    "print(f'TF Version:{tf.__version__}  |  GPU:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "from pathlib import Path\n",
    "base_folder = Path('.')\n",
    "data_folder = base_folder/'til2020'\n",
    "train_imgs_folder = data_folder/'train'/'train'\n",
    "train_annotations = data_folder/'train.json'\n",
    "val_imgs_folder = data_folder/'val'/'val'\n",
    "val_annotations = data_folder/'val.json'\n",
    "\n",
    "train_pickle = data_folder/'train.p'/'train.p'\n",
    "val_pickle = data_folder/'val.p'/'val.p'\n",
    "\n",
    "save_model_folder = base_folder/'ckpts'\n",
    "load_model_folder = base_folder/'ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n",
    "\n",
    "    input_shape = (224,224,3)\n",
    "    wt_decay = 5e-4\n",
    "\n",
    "    dims_list = [(7,7),(14,14)]\n",
    "    aspect_ratios = [(1,1), (1,2), (2,1)]\n",
    "\n",
    "    batch_size = 16\n",
    "    epoch_warmup = 300\n",
    "    epoch_finetune = 300\n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "# Shape of ypred: ( batch, i, j, aspect_ratios, 1+4+numclasses ). For a batch,i,j, we get #aspect_ratios vectors of length 7.\n",
    "# Shape of ytrue: ( batch, i, j, aspect_ratios, 1+4+numclasses+2 ). For a batch,i,j, we get #aspect_ratios vectors of length 9 (two more for objectness and cat/loc indicators)\n",
    "#TODO Play with weights?\n",
    "def custom_loss(ytrue, ypred):\n",
    "    obj_loss_weight = 1.0\n",
    "    cat_loss_weight = 1.0\n",
    "    loc_loss_weight = 1.0\n",
    "\n",
    "    end_cat = len(conf.cat_list) + 1\n",
    "\n",
    "    objloss_indicators = ytrue[:,:,:,:,-2:-1]\n",
    "    catlocloss_indicators = ytrue[:,:,:,:,-1:]\n",
    "\n",
    "    ytrue_obj, ypred_obj = ytrue[:,:,:,:,:1], ypred[:,:,:,:,:1]\n",
    "    ytrue_obj = tf.where( objloss_indicators != 0, ytrue_obj, 0 )\n",
    "    ypred_obj = tf.where( objloss_indicators != 0, ypred_obj, 0 )\n",
    "    objectness_loss = losses.BinaryCrossentropy(from_logits=True)( ytrue_obj, ypred_obj )\n",
    "\n",
    "    ytrue_cat, ypred_cat = ytrue[:,:,:,:,1:end_cat], ypred[:,:,:,:,1:end_cat]\n",
    "    ytrue_cat = tf.where( catlocloss_indicators != 0, ytrue_cat, 0 )\n",
    "    ypred_cat = tf.where( catlocloss_indicators != 0, ypred_cat, 0 )\n",
    "    categorical_loss = losses.CategoricalCrossentropy(from_logits=True) ( ytrue_cat, ypred_cat )\n",
    "\n",
    "    # Remember that ytrue is longer than ypred, so we will need to stop at index -2, which is where the indicators are stored\n",
    "    ytrue_loc, ypred_loc = ytrue[:,:,:,:,end_cat:-2], ypred[:,:,:,:,end_cat:]\n",
    "    ytrue_loc = tf.where( catlocloss_indicators != 0, ytrue_loc, 0 )\n",
    "    ypred_loc = tf.where( catlocloss_indicators != 0, ypred_loc, 0 )\n",
    "    localisation_loss = losses.Huber() ( ytrue_loc, ypred_loc )\n",
    "\n",
    "    return obj_loss_weight*objectness_loss + cat_loss_weight*categorical_loss + loc_loss_weight*localisation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ah functional paradigm\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "#I wrote this to reduce code size (sequential layers with activation of ReLU)\n",
    "def seq_with_activation(lst):\n",
    "    def wrapper(x):\n",
    "        nonlocal lst\n",
    "        try: iter(lst)\n",
    "        except TypeError: lst = [lst]\n",
    "        for l in lst:\n",
    "            x = l(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.LeakyReLU(0.01)(x)\n",
    "        return x\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def transfer_model_7x7_14x14(backbone_model, input_shape, dims_list, num_aspect_ratios, num_classes, wt_decay, model_name='transfer-objdet-model-7x7-14x14'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    intermediate_layer_model = keras.Model(inputs=backbone_model.input,\n",
    "        #outputs=backbone_model.get_layer('conv4_block6_out').output #Resnet50\n",
    "        outputs=backbone_model.get_layer('block13_sepconv2_bn').output #Xceptionnet, copied example in picking last layer of res 14\n",
    "        #TODO: PUT MORE THOUGHT INTO WHICH LAYER TO PICK BY INVESTIGATING ACTIVATIONS\n",
    "    )\n",
    "\n",
    "    intermediate_output = intermediate_layer_model(inputs) #14\n",
    "    backbone_output = backbone_model(inputs) #7\n",
    "\n",
    "    #TODO: not copy example, and strategize our own stuff\n",
    "    upsample = seq_with_activation([\n",
    "        layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "    ])(backbone_output)\n",
    "\n",
    "    x = seq_with_activation([\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2DTranspose(512, 5, strides=(2, 2), padding='same'), #14\n",
    "    ])(upsample)\n",
    "    x = layers.Concatenate()([x,intermediate_output])\n",
    "\n",
    "    tens_14x14 = seq_with_activation([\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "    ])(x)\n",
    "\n",
    "    tens_7x7 = layers.Add()([\n",
    "        seq_with_activation(layers.Conv2D(2048, 3, padding='same', kernel_regularizer=l2(wt_decay)))(upsample),\n",
    "        backbone_output\n",
    "    ])\n",
    "\n",
    "    dim_tensor_map = {'7x7':tens_7x7,'14x14':tens_14x14}\n",
    "\n",
    "    #Accumulate predictions for 7x7,14x14 into a dictionary for keras multi labels.\n",
    "    preds_dict = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        tens = dim_tensor_map[dimkey]\n",
    "        ar_preds = []\n",
    "        for _ in range(num_aspect_ratios):\n",
    "            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            class_preds = layers.Conv2D(num_classes, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n",
    "\n",
    "        if num_aspect_ratios > 1: predictions = layers.Concatenate()(ar_preds)\n",
    "        elif num_aspect_ratios == 1: predictions = ar_preds[0]\n",
    "\n",
    "        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+num_classes), name=dimkey )(predictions)\n",
    "        preds_dict[dimkey] = predictions\n",
    "\n",
    "    model = keras.Model(inputs, preds_dict, name=model_name)\n",
    "\n",
    "    model.compile( optimizer=keras.optimizers.Adam(1e-5),\n",
    "                    loss=custom_loss )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_monitors(save_path):\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=save_path,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n",
    "    return [model_checkpoint_callback,earlystopping,reduce_lr]\n",
    "\n",
    "def train(model,backbone_name,warmup=True):\n",
    "    if warmup: \n",
    "        save_model_path = str(save_model_folder/f'pt-{model_context}-best_val_loss.h5')\n",
    "        for layer in model.get_layer(backbone_name).layers: layer.trainable = False #dont train pretrained during warm up\n",
    "    else:\n",
    "        save_model_path = str(save_model_folder/f'ft-{model_context}-best_val_loss.h5')\n",
    "        for layer in model.get_layer(backbone_name).layers: layer.trainable = True\n",
    "\n",
    "    model.fit(\n",
    "        x=train_sequence, \n",
    "        epochs=(conf.epoch_warmup if warmup else conf.epoch_finetune), \n",
    "        validation_data=val_sequence, \n",
    "        callbacks=model_monitors(save_model_path),\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model-7x7-14x14-3aspect-modyoloposneg-wd0.0005-xception\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nxception (Model)                (None, 7, 7, 2048)   20861480    input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 7, 7, 512)    1049088     xception[1][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 7, 7, 512)    2048        conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 7, 7, 512)    0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 7, 7, 1024)   4719616     leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 7, 7, 1024)   4096        conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)       (None, 7, 7, 1024)   0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 7, 7, 512)    524800      leaky_re_lu_1[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 7, 7, 512)    2048        conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 7, 7, 1024)   4719616     leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 7, 7, 1024)   4096        conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 7, 7, 1024)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 7, 7, 512)    524800      leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 7, 7, 512)    2048        conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 7, 7, 256)    131328      leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 7, 7, 256)    1024        conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 7, 7, 256)    0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 14, 14, 512)  3277312     leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 14, 14, 512)  2048        conv2d_transpose[0][0]           \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, 14, 14, 512)  0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nmodel (Model)                   (None, 14, 14, 1024) 15355944    input_2[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 14, 14, 1536) 0           leaky_re_lu_6[0][0]              \n                                                                 model[1][0]                      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 14, 14, 256)  393472      concatenate[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 14, 14, 256)  1024        conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)       (None, 14, 14, 256)  0           batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_7[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 14, 14, 512)  2048        conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 512)  0           batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 14, 14, 256)  131328      leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 14, 14, 256)  1024        conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)       (None, 14, 14, 256)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_9[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 14, 14, 512)  2048        conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)      (None, 14, 14, 512)  0           batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 14, 14, 256)  131328      leaky_re_lu_10[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 14, 14, 256)  1024        conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 7, 7, 2048)   9439232     leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_11[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 7, 7, 2048)   8192        conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 14, 14, 512)  2048        conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)      (None, 7, 7, 2048)   0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)      (None, 14, 14, 512)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nadd_12 (Add)                    (None, 7, 7, 2048)   0           leaky_re_lu_13[0][0]             \n                                                                 xception[1][0]                   \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 7, 7, 1)      2049        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 7, 7, 5)      10245       add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 7, 7, 4)      8196        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 7, 7, 1)      2049        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 7, 7, 5)      10245       add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 7, 7, 4)      8196        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 7, 7, 1)      2049        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 7, 7, 5)      10245       add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 7, 7, 4)      8196        add_12[0][0]                     \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 14, 14, 10)   0           conv2d_26[0][0]                  \n                                                                 conv2d_27[0][0]                  \n                                                                 conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 14, 14, 10)   0           conv2d_29[0][0]                  \n                                                                 conv2d_30[0][0]                  \n                                                                 conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 14, 14, 10)   0           conv2d_32[0][0]                  \n                                                                 conv2d_33[0][0]                  \n                                                                 conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 7, 7, 10)     0           conv2d_17[0][0]                  \n                                                                 conv2d_18[0][0]                  \n                                                                 conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 7, 7, 10)     0           conv2d_20[0][0]                  \n                                                                 conv2d_21[0][0]                  \n                                                                 conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 7, 7, 10)     0           conv2d_23[0][0]                  \n                                                                 conv2d_24[0][0]                  \n                                                                 conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_8 (Concatenate)     (None, 14, 14, 30)   0           concatenate_5[0][0]              \n                                                                 concatenate_6[0][0]              \n                                                                 concatenate_7[0][0]              \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 7, 7, 30)     0           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n                                                                 concatenate_3[0][0]              \n__________________________________________________________________________________________________\n14x14 (Reshape)                 (None, 14, 14, 3, 10 0           concatenate_8[0][0]              \n__________________________________________________________________________________________________\n7x7 (Reshape)                   (None, 7, 7, 3, 10)  0           concatenate_4[0][0]              \n==================================================================================================\nTotal params: 49,555,556\nTrainable params: 49,483,620\nNon-trainable params: 71,936\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model_context = 'model-7x7-14x14-3aspect-modyoloposneg-wd{}'.format(conf.wt_decay)\n",
    "# load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.h5'.format(model_context) )\n",
    "load_model_path = None\n",
    "\n",
    "if load_model_path is None:\n",
    "    #backbone_model = keras.applications.ResNet50(input_shape=conf.input_shape,include_top=False)\n",
    "    backbone_model = keras.applications.Xception(input_shape=conf.input_shape,include_top=False)\n",
    "    model = transfer_model_7x7_14x14(backbone_model,\n",
    "        input_shape=conf.input_shape,\n",
    "        dims_list=conf.dims_list,\n",
    "        num_aspect_ratios=len(conf.aspect_ratios),\n",
    "        num_classes=len(conf.cat_list),\n",
    "        wt_decay=conf.wt_decay,\n",
    "        #model_name=model_context+'-res50'\n",
    "        model_name=model_context+'-xception'\n",
    "    )\n",
    "else:\n",
    "    model = keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.loader import TILSequence,TILPickle\n",
    "from scripts.sampling import iou,modified_yolo_posneg_sampling\n",
    "from scripts.augment import aug_default,aug_identity\n",
    "from scripts.encoder import encode_label\n",
    "\n",
    "label_encoder = lambda y: encode_label(y, conf.dims_list, conf.aspect_ratios, iou, modified_yolo_posneg_sampling, conf.cat_list)\n",
    "preproc_fn = lambda x: x / 255.\n",
    "\n",
    "if True:\n",
    "    train_sequence = TILPickle(train_pickle, conf.batch_size, aug_default, (224,224), label_encoder, preproc_fn)\n",
    "    val_sequence = TILPickle(val_pickle, conf.batch_size, aug_identity, (224,224), label_encoder, preproc_fn)\n",
    "else:\n",
    "    train_sequence = TILSequence(train_imgs_folder,train_annotations,conf.batch_size,aug_default,(224,224),label_encoder,preproc_fn)\n",
    "    val_sequence = TILSequence(val_imgs_folder,val_annotations,conf.batch_size,aug_identity,(224,224),label_encoder,preproc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Warming up the model...\nEpoch 1/300\n515/515 [==============================] - ETA: 0s - loss: 3.7852 - 14x14_loss: 0.2220 - 7x7_loss: 0.1532"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'WindowsPath' object has no attribute 'format'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b73ddf94365c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Warming up the model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'xception'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Fine tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model warmed. Loading best val version of model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-52c84e532cb3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, backbone_name, warmup)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackbone_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     model.fit(\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_sequence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_warmup\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwarmup\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_finetune\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TIL\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TIL\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TIL\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TIL\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1175\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TIL\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1194\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1195\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TIL\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[1;31m# `{mape:.2f}`. A mismatch between logged metrics and the path's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m         \u001b[1;31m# placeholders can cause formatting to fail.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m         raise KeyError('Failed to format this callback filepath: \"{}\". '\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WindowsPath' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "print('Warming up the model...')\n",
    "train(model,'xception',warmup=True)\n",
    "\n",
    "# Fine tuning\n",
    "print('Model warmed. Loading best val version of model...')\n",
    "del model\n",
    "load_model_path = load_model_folder/f'pt-{model_context}-best_val_loss.h5'\n",
    "model = keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),loss=custom_loss)\n",
    "train(model,'xception',warmup=False)\n",
    "\n",
    "# Final save\n",
    "model.save(os.path.join(save_model_folder, 'ft-{}-final.h5'.format(model_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 8s 0us/step\n"
    }
   ],
   "source": [
    "#TODO: import hyperopts. play with hyperopts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bittilconda431f6b117ae54518ae316f75f88a7342",
   "display_name": "Python 3.8.3 64-bit ('TIL': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}