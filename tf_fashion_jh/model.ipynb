{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "TF Version:2.2.0  |  GPU:PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "device = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(device, True)\n",
    "print(f'TF Version:{tf.__version__}  |  GPU:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "from pathlib import Path\n",
    "base_folder = Path('.')\n",
    "data_folder = base_folder/'til2020'\n",
    "train_imgs_folder = data_folder/'train'/'train'\n",
    "train_annotations = data_folder/'train.json'\n",
    "val_imgs_folder = data_folder/'val'/'val'\n",
    "val_annotations = data_folder/'val.json'\n",
    "\n",
    "train_pickle = data_folder/'train.p'/'train.p'\n",
    "val_pickle = data_folder/'val.p'/'val.p'\n",
    "\n",
    "save_model_folder = base_folder/'ckpts'\n",
    "load_model_folder = base_folder/'ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n",
    "\n",
    "    input_shape = (224,224,3)\n",
    "    wt_decay = 5e-4\n",
    "\n",
    "    dims_list = [(7,7),(14,14)]\n",
    "    aspect_ratios = [(1,1), (1,2), (2,1)]\n",
    "\n",
    "    batch_size = 16\n",
    "    epoch_warmup = 300\n",
    "    epoch_finetune = 300\n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "# Shape of ypred: ( batch, i, j, aspect_ratios, 1+4+numclasses ). For a batch,i,j, we get #aspect_ratios vectors of length 7.\n",
    "# Shape of ytrue: ( batch, i, j, aspect_ratios, 1+4+numclasses+2 ). For a batch,i,j, we get #aspect_ratios vectors of length 9 (two more for objectness and cat/loc indicators)\n",
    "#TODO Play with weights?\n",
    "def custom_loss(ytrue, ypred):\n",
    "    obj_loss_weight = 1.0\n",
    "    cat_loss_weight = 1.0\n",
    "    loc_loss_weight = 1.0\n",
    "\n",
    "    end_cat = len(conf.cat_list) + 1\n",
    "\n",
    "    objloss_indicators = ytrue[:,:,:,:,-2:-1]\n",
    "    catlocloss_indicators = ytrue[:,:,:,:,-1:]\n",
    "\n",
    "    ytrue_obj, ypred_obj = ytrue[:,:,:,:,:1], ypred[:,:,:,:,:1]\n",
    "    ytrue_obj = tf.where( objloss_indicators != 0, ytrue_obj, 0 )\n",
    "    ypred_obj = tf.where( objloss_indicators != 0, ypred_obj, 0 )\n",
    "    objectness_loss = losses.BinaryCrossentropy(from_logits=True)( ytrue_obj, ypred_obj )\n",
    "\n",
    "    ytrue_cat, ypred_cat = ytrue[:,:,:,:,1:end_cat], ypred[:,:,:,:,1:end_cat]\n",
    "    ytrue_cat = tf.where( catlocloss_indicators != 0, ytrue_cat, 0 )\n",
    "    ypred_cat = tf.where( catlocloss_indicators != 0, ypred_cat, 0 )\n",
    "    categorical_loss = losses.CategoricalCrossentropy(from_logits=True) ( ytrue_cat, ypred_cat )\n",
    "\n",
    "    # Remember that ytrue is longer than ypred, so we will need to stop at index -2, which is where the indicators are stored\n",
    "    ytrue_loc, ypred_loc = ytrue[:,:,:,:,end_cat:-2], ypred[:,:,:,:,end_cat:]\n",
    "    ytrue_loc = tf.where( catlocloss_indicators != 0, ytrue_loc, 0 )\n",
    "    ypred_loc = tf.where( catlocloss_indicators != 0, ypred_loc, 0 )\n",
    "    localisation_loss = losses.Huber() ( ytrue_loc, ypred_loc )\n",
    "\n",
    "    return obj_loss_weight*objectness_loss + cat_loss_weight*categorical_loss + loc_loss_weight*localisation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ah functional paradigm\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "#I wrote this to reduce code size (sequential layers with activation of ReLU)\n",
    "def seq_with_activation(lst):\n",
    "    def wrapper(x):\n",
    "        nonlocal lst\n",
    "        try: iter(lst)\n",
    "        except TypeError: lst = [lst]\n",
    "        for l in lst:\n",
    "            x = l(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.LeakyReLU(0.01)(x)\n",
    "        return x\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def transfer_model_7x7_14x14(backbone_model, input_shape, dims_list, num_aspect_ratios, num_classes, wt_decay, model_name='transfer-objdet-model-7x7-14x14'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    intermediate_layer_model = keras.Model(inputs=backbone_model.input,\n",
    "        #outputs=backbone_model.get_layer('conv4_block6_out').output #Resnet50\n",
    "        outputs=backbone_model.get_layer('block13_sepconv2_bn').output #Xceptionnet, copied example in picking last layer of res 14\n",
    "        #TODO: PUT MORE THOUGHT INTO WHICH LAYER TO PICK BY INVESTIGATING ACTIVATIONS\n",
    "    )\n",
    "\n",
    "    intermediate_output = intermediate_layer_model(inputs) #14\n",
    "    backbone_output = backbone_model(inputs) #7\n",
    "\n",
    "    #TODO: not copy example, and strategize our own stuff\n",
    "    upsample = seq_with_activation([\n",
    "        layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "    ])(backbone_output)\n",
    "\n",
    "    x = seq_with_activation([\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #7\n",
    "        layers.Conv2DTranspose(512, 5, strides=(2, 2), padding='same'), #14\n",
    "    ])(upsample)\n",
    "    x = layers.Concatenate()([x,intermediate_output])\n",
    "\n",
    "    tens_14x14 = seq_with_activation([\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "        layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay)), #14\n",
    "    ])(x)\n",
    "\n",
    "    tens_7x7 = layers.Add()([\n",
    "        seq_with_activation(layers.Conv2D(2048, 3, padding='same', kernel_regularizer=l2(wt_decay)))(upsample),\n",
    "        backbone_output\n",
    "    ])\n",
    "\n",
    "    dim_tensor_map = {'7x7':tens_7x7,'14x14':tens_14x14}\n",
    "\n",
    "    #Accumulate predictions for 7x7,14x14 into a dictionary for keras multi labels.\n",
    "    preds_dict = {}\n",
    "    for dims in dims_list:\n",
    "        dimkey = '{}x{}'.format(*dims)\n",
    "        tens = dim_tensor_map[dimkey]\n",
    "        ar_preds = []\n",
    "        for _ in range(num_aspect_ratios):\n",
    "            objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            class_preds = layers.Conv2D(num_classes, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
    "            ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n",
    "\n",
    "        if num_aspect_ratios > 1: predictions = layers.Concatenate()(ar_preds)\n",
    "        elif num_aspect_ratios == 1: predictions = ar_preds[0]\n",
    "\n",
    "        predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+num_classes), name=dimkey )(predictions)\n",
    "        preds_dict[dimkey] = predictions\n",
    "\n",
    "    model = keras.Model(inputs, preds_dict, name=model_name)\n",
    "\n",
    "    model.compile( optimizer=keras.optimizers.Adam(1e-5),\n",
    "                    loss=custom_loss )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_monitors(save_path):\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=save_path,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8)\n",
    "    return [model_checkpoint_callback,earlystopping,reduce_lr]\n",
    "\n",
    "def train(model,backbone_name,warmup=True):\n",
    "    if warmup: \n",
    "        save_model_path = str(save_model_folder/f'pt-{model_context}-best_val_loss.ckpt')\n",
    "        for layer in model.get_layer(backbone_name).layers: layer.trainable = False #dont train pretrained during warm up\n",
    "    else:\n",
    "        save_model_path = str(save_model_folder/f'ft-{model_context}-best_val_loss.ckpt')\n",
    "        for layer in model.get_layer(backbone_name).layers: layer.trainable = True\n",
    "\n",
    "    model.fit(\n",
    "        x=train_sequence, \n",
    "        epochs=(conf.epoch_warmup if warmup else conf.epoch_finetune), \n",
    "        validation_data=val_sequence, \n",
    "        callbacks=model_monitors(save_model_path),\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model-7x7-14x14-3aspect-modyoloposneg-wd0.0005-xception\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nxception (Model)                (None, 7, 7, 2048)   20861480    input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 7, 7, 512)    1049088     xception[1][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 7, 7, 512)    2048        conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 7, 7, 512)    0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 7, 7, 1024)   4719616     leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 7, 7, 1024)   4096        conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)       (None, 7, 7, 1024)   0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 7, 7, 512)    524800      leaky_re_lu_1[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 7, 7, 512)    2048        conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 7, 7, 1024)   4719616     leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 7, 7, 1024)   4096        conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 7, 7, 1024)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 7, 7, 512)    524800      leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 7, 7, 512)    2048        conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 7, 7, 256)    131328      leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 7, 7, 256)    1024        conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 7, 7, 256)    0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_transpose (Conv2DTranspo (None, 14, 14, 512)  3277312     leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 14, 14, 512)  2048        conv2d_transpose[0][0]           \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, 14, 14, 512)  0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nmodel (Model)                   (None, 14, 14, 1024) 15355944    input_2[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 14, 14, 1536) 0           leaky_re_lu_6[0][0]              \n                                                                 model[1][0]                      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 14, 14, 256)  393472      concatenate[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 14, 14, 256)  1024        conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)       (None, 14, 14, 256)  0           batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_7[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 14, 14, 512)  2048        conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 512)  0           batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 14, 14, 256)  131328      leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 14, 14, 256)  1024        conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)       (None, 14, 14, 256)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_9[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 14, 14, 512)  2048        conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)      (None, 14, 14, 512)  0           batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 14, 14, 256)  131328      leaky_re_lu_10[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 14, 14, 256)  1024        conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 7, 7, 2048)   9439232     leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_11[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 7, 7, 2048)   8192        conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 14, 14, 512)  2048        conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)      (None, 7, 7, 2048)   0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)      (None, 14, 14, 512)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nadd_12 (Add)                    (None, 7, 7, 2048)   0           leaky_re_lu_13[0][0]             \n                                                                 xception[1][0]                   \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 7, 7, 1)      2049        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 7, 7, 5)      10245       add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 7, 7, 4)      8196        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 7, 7, 1)      2049        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 7, 7, 5)      10245       add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 7, 7, 4)      8196        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 7, 7, 1)      2049        add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 7, 7, 5)      10245       add_12[0][0]                     \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 7, 7, 4)      8196        add_12[0][0]                     \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 14, 14, 10)   0           conv2d_26[0][0]                  \n                                                                 conv2d_27[0][0]                  \n                                                                 conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 14, 14, 10)   0           conv2d_29[0][0]                  \n                                                                 conv2d_30[0][0]                  \n                                                                 conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 14, 14, 10)   0           conv2d_32[0][0]                  \n                                                                 conv2d_33[0][0]                  \n                                                                 conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 7, 7, 10)     0           conv2d_17[0][0]                  \n                                                                 conv2d_18[0][0]                  \n                                                                 conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 7, 7, 10)     0           conv2d_20[0][0]                  \n                                                                 conv2d_21[0][0]                  \n                                                                 conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 7, 7, 10)     0           conv2d_23[0][0]                  \n                                                                 conv2d_24[0][0]                  \n                                                                 conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_8 (Concatenate)     (None, 14, 14, 30)   0           concatenate_5[0][0]              \n                                                                 concatenate_6[0][0]              \n                                                                 concatenate_7[0][0]              \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 7, 7, 30)     0           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n                                                                 concatenate_3[0][0]              \n__________________________________________________________________________________________________\n14x14 (Reshape)                 (None, 14, 14, 3, 10 0           concatenate_8[0][0]              \n__________________________________________________________________________________________________\n7x7 (Reshape)                   (None, 7, 7, 3, 10)  0           concatenate_4[0][0]              \n==================================================================================================\nTotal params: 49,555,556\nTrainable params: 49,483,620\nNon-trainable params: 71,936\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model_context = f'model-7x7-14x14-3aspect-modyoloposneg-wd{conf.wt_decay}'\n",
    "#load_model_path = load_model_folder/f'ft-{model_context}-best_val_loss.ckpt'\n",
    "#load_model_path = load_model_folder/f'pt-{model_context}-best_val_loss.ckpt'\n",
    "#load_model_path = None\n",
    "\n",
    "if load_model_path is None:\n",
    "    #backbone_model = keras.applications.ResNet50(input_shape=conf.input_shape,include_top=False)\n",
    "    backbone_model = keras.applications.Xception(input_shape=conf.input_shape,include_top=False)\n",
    "    model = transfer_model_7x7_14x14(backbone_model,\n",
    "        input_shape=conf.input_shape,\n",
    "        dims_list=conf.dims_list,\n",
    "        num_aspect_ratios=len(conf.aspect_ratios),\n",
    "        num_classes=len(conf.cat_list),\n",
    "        wt_decay=conf.wt_decay,\n",
    "        #model_name=model_context+'-res50'\n",
    "        model_name=model_context+'-xception'\n",
    "    )\n",
    "else:\n",
    "    model = keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.loader import TILSequence,TILPickle\n",
    "from scripts.sampling import iou,modified_yolo_posneg_sampling\n",
    "from scripts.augment import aug_default,aug_identity\n",
    "from scripts.encoder import encode_label\n",
    "\n",
    "label_encoder = lambda y: encode_label(y, conf.dims_list, conf.aspect_ratios, iou, modified_yolo_posneg_sampling, conf.cat_list)\n",
    "preproc_fn = lambda x: x / 255.\n",
    "\n",
    "if True:\n",
    "    train_sequence = TILPickle(train_pickle, conf.batch_size, aug_default, conf.input_shape[:-1], label_encoder, preproc_fn)\n",
    "    val_sequence = TILPickle(val_pickle, conf.batch_size, aug_identity, conf.input_shape[:-1], label_encoder, preproc_fn)\n",
    "else:\n",
    "    train_sequence = TILSequence(train_imgs_folder,train_annotations,conf.batch_size,aug_default,conf.input_shape[:-1],label_encoder,preproc_fn)\n",
    "    val_sequence = TILSequence(val_imgs_folder,val_annotations,conf.batch_size,aug_identity,conf.input_shape[:-1],label_encoder,preproc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Warming up the model...\nEpoch 1/300\n515/515 [==============================] - 282s 549ms/step - loss: 0.0903 - 14x14_loss: 8.1744e-04 - 7x7_loss: 0.0111 - val_loss: 0.1451 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0665 - lr: 1.0000e-05\nEpoch 2/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0875 - 14x14_loss: 8.0028e-04 - 7x7_loss: 0.0104 - val_loss: 0.1600 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0836 - lr: 1.0000e-05\nEpoch 3/300\n515/515 [==============================] - 216s 419ms/step - loss: 0.0855 - 14x14_loss: 8.0983e-04 - 7x7_loss: 0.0106 - val_loss: 0.1507 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0763 - lr: 1.0000e-05\nEpoch 4/300\n515/515 [==============================] - 217s 421ms/step - loss: 0.0837 - 14x14_loss: 7.8983e-04 - 7x7_loss: 0.0107 - val_loss: 0.1482 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0758 - lr: 1.0000e-05\nEpoch 5/300\n515/515 [==============================] - 276s 536ms/step - loss: 0.0819 - 14x14_loss: 8.3014e-04 - 7x7_loss: 0.0108 - val_loss: 0.1416 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0710 - lr: 1.0000e-05\nEpoch 6/300\n515/515 [==============================] - 275s 535ms/step - loss: 0.0796 - 14x14_loss: 7.9216e-04 - 7x7_loss: 0.0104 - val_loss: 0.1389 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0702 - lr: 1.0000e-05\nEpoch 7/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0779 - 14x14_loss: 8.2149e-04 - 7x7_loss: 0.0105 - val_loss: 0.1426 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0757 - lr: 1.0000e-05\nEpoch 8/300\n515/515 [==============================] - 215s 418ms/step - loss: 0.0758 - 14x14_loss: 8.0062e-04 - 7x7_loss: 0.0102 - val_loss: 0.1473 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0822 - lr: 1.0000e-05\nEpoch 9/300\n515/515 [==============================] - 273s 531ms/step - loss: 0.0740 - 14x14_loss: 7.8036e-04 - 7x7_loss: 0.0102 - val_loss: 0.1353 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0719 - lr: 1.0000e-05\nEpoch 10/300\n515/515 [==============================] - 277s 538ms/step - loss: 0.0723 - 14x14_loss: 7.7102e-04 - 7x7_loss: 0.0100 - val_loss: 0.1317 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0699 - lr: 1.0000e-05\nEpoch 11/300\n515/515 [==============================] - 276s 536ms/step - loss: 0.0709 - 14x14_loss: 7.7916e-04 - 7x7_loss: 0.0102 - val_loss: 0.1312 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0709 - lr: 1.0000e-05\nEpoch 12/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0700 - 14x14_loss: 7.5708e-04 - 7x7_loss: 0.0108 - val_loss: 0.1426 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0836 - lr: 1.0000e-05\nEpoch 13/300\n515/515 [==============================] - 275s 535ms/step - loss: 0.0678 - 14x14_loss: 7.9197e-04 - 7x7_loss: 0.0099 - val_loss: 0.1284 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0708 - lr: 1.0000e-05\nEpoch 14/300\n515/515 [==============================] - 218s 423ms/step - loss: 0.0661 - 14x14_loss: 7.5751e-04 - 7x7_loss: 0.0096 - val_loss: 0.1329 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0767 - lr: 1.0000e-05\nEpoch 15/300\n515/515 [==============================] - 287s 558ms/step - loss: 0.0645 - 14x14_loss: 7.8476e-04 - 7x7_loss: 0.0094 - val_loss: 0.1213 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0665 - lr: 1.0000e-05\nEpoch 16/300\n515/515 [==============================] - 225s 437ms/step - loss: 0.0635 - 14x14_loss: 7.5499e-04 - 7x7_loss: 0.0098 - val_loss: 0.1312 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0776 - lr: 1.0000e-05\nEpoch 17/300\n515/515 [==============================] - 225s 437ms/step - loss: 0.0626 - 14x14_loss: 7.8997e-04 - 7x7_loss: 0.0101 - val_loss: 0.1256 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0734 - lr: 1.0000e-05\nEpoch 18/300\n515/515 [==============================] - 225s 437ms/step - loss: 0.0610 - 14x14_loss: 7.4395e-04 - 7x7_loss: 0.0098 - val_loss: 0.1385 - val_14x14_loss: 0.0014 - val_7x7_loss: 0.0872 - lr: 1.0000e-05\nEpoch 19/300\n515/515 [==============================] - 227s 441ms/step - loss: 0.0601 - 14x14_loss: 7.7179e-04 - 7x7_loss: 0.0100 - val_loss: 0.1275 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0775 - lr: 1.0000e-05\nEpoch 20/300\n515/515 [==============================] - 227s 440ms/step - loss: 0.0588 - 14x14_loss: 7.8793e-04 - 7x7_loss: 0.0099 - val_loss: 0.1260 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0772 - lr: 1.0000e-05\nEpoch 21/300\n515/515 [==============================] - 227s 442ms/step - loss: 0.0567 - 14x14_loss: 6.8887e-04 - 7x7_loss: 0.0086 - val_loss: 0.1243 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0759 - lr: 2.0000e-06\nEpoch 22/300\n515/515 [==============================] - 225s 437ms/step - loss: 0.0558 - 14x14_loss: 6.3935e-04 - 7x7_loss: 0.0081 - val_loss: 0.1216 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0736 - lr: 2.0000e-06\nEpoch 23/300\n515/515 [==============================] - 229s 444ms/step - loss: 0.0553 - 14x14_loss: 6.7095e-04 - 7x7_loss: 0.0080 - val_loss: 0.1236 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0759 - lr: 2.0000e-06\nEpoch 24/300\n515/515 [==============================] - 228s 443ms/step - loss: 0.0546 - 14x14_loss: 6.1373e-04 - 7x7_loss: 0.0077 - val_loss: 0.1234 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0761 - lr: 2.0000e-06\nEpoch 25/300\n515/515 [==============================] - 292s 566ms/step - loss: 0.0544 - 14x14_loss: 6.2197e-04 - 7x7_loss: 0.0078 - val_loss: 0.1203 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0734 - lr: 2.0000e-06\nEpoch 26/300\n515/515 [==============================] - 227s 440ms/step - loss: 0.0542 - 14x14_loss: 6.2130e-04 - 7x7_loss: 0.0079 - val_loss: 0.1254 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0789 - lr: 2.0000e-06\nEpoch 27/300\n515/515 [==============================] - 228s 443ms/step - loss: 0.0537 - 14x14_loss: 6.6736e-04 - 7x7_loss: 0.0078 - val_loss: 0.1222 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0759 - lr: 2.0000e-06\nEpoch 28/300\n515/515 [==============================] - 228s 443ms/step - loss: 0.0534 - 14x14_loss: 6.4157e-04 - 7x7_loss: 0.0079 - val_loss: 0.1238 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0779 - lr: 2.0000e-06\nEpoch 29/300\n515/515 [==============================] - 227s 441ms/step - loss: 0.0525 - 14x14_loss: 6.0979e-04 - 7x7_loss: 0.0074 - val_loss: 0.1212 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0757 - lr: 2.0000e-06\nEpoch 30/300\n515/515 [==============================] - 227s 442ms/step - loss: 0.0520 - 14x14_loss: 5.9834e-04 - 7x7_loss: 0.0073 - val_loss: 0.1207 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0756 - lr: 2.0000e-06\nEpoch 31/300\n515/515 [==============================] - 291s 564ms/step - loss: 0.0517 - 14x14_loss: 6.0319e-04 - 7x7_loss: 0.0072 - val_loss: 0.1197 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 4.0000e-07\nEpoch 32/300\n515/515 [==============================] - 222s 431ms/step - loss: 0.0518 - 14x14_loss: 6.0479e-04 - 7x7_loss: 0.0074 - val_loss: 0.1208 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0758 - lr: 4.0000e-07\nEpoch 33/300\n515/515 [==============================] - 212s 412ms/step - loss: 0.0519 - 14x14_loss: 6.0478e-04 - 7x7_loss: 0.0076 - val_loss: 0.1206 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0757 - lr: 4.0000e-07\nEpoch 34/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0514 - 14x14_loss: 6.3578e-04 - 7x7_loss: 0.0071 - val_loss: 0.1206 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0758 - lr: 4.0000e-07\nEpoch 35/300\n515/515 [==============================] - 225s 436ms/step - loss: 0.0511 - 14x14_loss: 6.0045e-04 - 7x7_loss: 0.0069 - val_loss: 0.1201 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0754 - lr: 4.0000e-07\nEpoch 36/300\n515/515 [==============================] - 212s 412ms/step - loss: 0.0513 - 14x14_loss: 5.7797e-04 - 7x7_loss: 0.0072 - val_loss: 0.1201 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0754 - lr: 4.0000e-07\nEpoch 37/300\n515/515 [==============================] - 273s 530ms/step - loss: 0.0511 - 14x14_loss: 6.1846e-04 - 7x7_loss: 0.0071 - val_loss: 0.1193 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 8.0000e-08\nEpoch 38/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0513 - 14x14_loss: 6.1107e-04 - 7x7_loss: 0.0072 - val_loss: 0.1194 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 8.0000e-08\nEpoch 39/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0515 - 14x14_loss: 6.3626e-04 - 7x7_loss: 0.0074 - val_loss: 0.1195 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0750 - lr: 8.0000e-08\nEpoch 40/300\n515/515 [==============================] - 274s 532ms/step - loss: 0.0510 - 14x14_loss: 6.0469e-04 - 7x7_loss: 0.0070 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0743 - lr: 8.0000e-08\nEpoch 41/300\n515/515 [==============================] - 277s 538ms/step - loss: 0.0511 - 14x14_loss: 6.0896e-04 - 7x7_loss: 0.0071 - val_loss: 0.1188 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0742 - lr: 8.0000e-08\nEpoch 42/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 6.0175e-04 - 7x7_loss: 0.0068 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0744 - lr: 8.0000e-08\nEpoch 43/300\n515/515 [==============================] - 278s 540ms/step - loss: 0.0508 - 14x14_loss: 6.1274e-04 - 7x7_loss: 0.0069 - val_loss: 0.1183 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0738 - lr: 8.0000e-08\nEpoch 44/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 6.0790e-04 - 7x7_loss: 0.0069 - val_loss: 0.1195 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0750 - lr: 8.0000e-08\nEpoch 45/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0507 - 14x14_loss: 5.7602e-04 - 7x7_loss: 0.0068 - val_loss: 0.1193 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0749 - lr: 8.0000e-08\nEpoch 46/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0508 - 14x14_loss: 5.8113e-04 - 7x7_loss: 0.0069 - val_loss: 0.1191 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 8.0000e-08\nEpoch 47/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0510 - 14x14_loss: 5.9228e-04 - 7x7_loss: 0.0071 - val_loss: 0.1192 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 8.0000e-08\nEpoch 48/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 5.8143e-04 - 7x7_loss: 0.0070 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0745 - lr: 8.0000e-08\nEpoch 49/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 5.9788e-04 - 7x7_loss: 0.0070 - val_loss: 0.1188 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0744 - lr: 1.6000e-08\nEpoch 50/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0511 - 14x14_loss: 5.9386e-04 - 7x7_loss: 0.0073 - val_loss: 0.1194 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0750 - lr: 1.6000e-08\nEpoch 51/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0508 - 14x14_loss: 5.9053e-04 - 7x7_loss: 0.0070 - val_loss: 0.1192 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 1.6000e-08\nEpoch 52/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 5.8785e-04 - 7x7_loss: 0.0071 - val_loss: 0.1196 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0752 - lr: 1.6000e-08\nEpoch 53/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0512 - 14x14_loss: 5.7059e-04 - 7x7_loss: 0.0074 - val_loss: 0.1191 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 1.6000e-08\nEpoch 54/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 6.0390e-04 - 7x7_loss: 0.0071 - val_loss: 0.1191 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 1.0000e-08\nEpoch 55/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 5.9299e-04 - 7x7_loss: 0.0069 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0745 - lr: 1.0000e-08\nEpoch 56/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0510 - 14x14_loss: 6.2240e-04 - 7x7_loss: 0.0071 - val_loss: 0.1183 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0739 - lr: 1.0000e-08\nEpoch 57/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0509 - 14x14_loss: 6.0078e-04 - 7x7_loss: 0.0071 - val_loss: 0.1193 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0749 - lr: 1.0000e-08\nEpoch 58/300\n515/515 [==============================] - 216s 420ms/step - loss: 0.0507 - 14x14_loss: 6.0585e-04 - 7x7_loss: 0.0068 - val_loss: 0.1183 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0739 - lr: 1.0000e-08\nEpoch 59/300\n515/515 [==============================] - 275s 534ms/step - loss: 0.0509 - 14x14_loss: 5.7397e-04 - 7x7_loss: 0.0071 - val_loss: 0.1183 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0739 - lr: 1.0000e-08\nEpoch 60/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 5.8909e-04 - 7x7_loss: 0.0070 - val_loss: 0.1184 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0740 - lr: 1.0000e-08\nEpoch 61/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0512 - 14x14_loss: 6.2819e-04 - 7x7_loss: 0.0073 - val_loss: 0.1195 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0751 - lr: 1.0000e-08\nEpoch 62/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 5.7796e-04 - 7x7_loss: 0.0070 - val_loss: 0.1197 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0753 - lr: 1.0000e-08\nEpoch 63/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0507 - 14x14_loss: 5.9060e-04 - 7x7_loss: 0.0069 - val_loss: 0.1183 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0739 - lr: 1.0000e-08\nEpoch 64/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 6.2024e-04 - 7x7_loss: 0.0069 - val_loss: 0.1191 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 1.0000e-08\nEpoch 65/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0511 - 14x14_loss: 5.8859e-04 - 7x7_loss: 0.0073 - val_loss: 0.1191 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 1.0000e-08\nEpoch 66/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0511 - 14x14_loss: 5.9360e-04 - 7x7_loss: 0.0073 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0745 - lr: 1.0000e-08\nEpoch 67/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0508 - 14x14_loss: 6.0655e-04 - 7x7_loss: 0.0070 - val_loss: 0.1193 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0750 - lr: 1.0000e-08\nEpoch 68/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0511 - 14x14_loss: 5.9035e-04 - 7x7_loss: 0.0073 - val_loss: 0.1198 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0754 - lr: 1.0000e-08\nEpoch 69/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0507 - 14x14_loss: 6.0110e-04 - 7x7_loss: 0.0069 - val_loss: 0.1194 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0750 - lr: 1.0000e-08\nEpoch 70/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0513 - 14x14_loss: 5.9925e-04 - 7x7_loss: 0.0075 - val_loss: 0.1196 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0752 - lr: 1.0000e-08\nEpoch 71/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0510 - 14x14_loss: 5.9196e-04 - 7x7_loss: 0.0072 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0745 - lr: 1.0000e-08\nEpoch 72/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0506 - 14x14_loss: 5.9819e-04 - 7x7_loss: 0.0068 - val_loss: 0.1184 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0740 - lr: 1.0000e-08\nEpoch 73/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0509 - 14x14_loss: 6.0862e-04 - 7x7_loss: 0.0071 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0745 - lr: 1.0000e-08\nEpoch 74/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0506 - 14x14_loss: 5.7829e-04 - 7x7_loss: 0.0068 - val_loss: 0.1192 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 1.0000e-08\nEpoch 75/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 6.0351e-04 - 7x7_loss: 0.0070 - val_loss: 0.1190 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0746 - lr: 1.0000e-08\nEpoch 76/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0508 - 14x14_loss: 5.9553e-04 - 7x7_loss: 0.0070 - val_loss: 0.1190 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 1.0000e-08\nEpoch 77/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0509 - 14x14_loss: 5.8690e-04 - 7x7_loss: 0.0071 - val_loss: 0.1190 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 1.0000e-08\nEpoch 78/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0511 - 14x14_loss: 6.2151e-04 - 7x7_loss: 0.0073 - val_loss: 0.1198 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0755 - lr: 1.0000e-08\nEpoch 79/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0506 - 14x14_loss: 5.9169e-04 - 7x7_loss: 0.0068 - val_loss: 0.1188 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0744 - lr: 1.0000e-08\nEpoch 80/300\n515/515 [==============================] - 278s 541ms/step - loss: 0.0508 - 14x14_loss: 5.6622e-04 - 7x7_loss: 0.0071 - val_loss: 0.1178 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0735 - lr: 1.0000e-08\nEpoch 81/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 5.9460e-04 - 7x7_loss: 0.0070 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0746 - lr: 1.0000e-08\nEpoch 82/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0509 - 14x14_loss: 5.9951e-04 - 7x7_loss: 0.0071 - val_loss: 0.1196 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0753 - lr: 1.0000e-08\nEpoch 83/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0511 - 14x14_loss: 5.8543e-04 - 7x7_loss: 0.0073 - val_loss: 0.1191 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 1.0000e-08\nEpoch 84/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0509 - 14x14_loss: 6.0357e-04 - 7x7_loss: 0.0072 - val_loss: 0.1193 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0750 - lr: 1.0000e-08\nEpoch 85/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0507 - 14x14_loss: 5.9721e-04 - 7x7_loss: 0.0069 - val_loss: 0.1185 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0742 - lr: 1.0000e-08\nEpoch 86/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0507 - 14x14_loss: 5.8939e-04 - 7x7_loss: 0.0069 - val_loss: 0.1185 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0742 - lr: 1.0000e-08\nEpoch 87/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0505 - 14x14_loss: 5.7342e-04 - 7x7_loss: 0.0068 - val_loss: 0.1182 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0739 - lr: 1.0000e-08\nEpoch 88/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0507 - 14x14_loss: 5.8724e-04 - 7x7_loss: 0.0070 - val_loss: 0.1194 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0751 - lr: 1.0000e-08\nEpoch 89/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0509 - 14x14_loss: 5.8696e-04 - 7x7_loss: 0.0071 - val_loss: 0.1188 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0745 - lr: 1.0000e-08\nEpoch 90/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0509 - 14x14_loss: 5.7845e-04 - 7x7_loss: 0.0071 - val_loss: 0.1182 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0739 - lr: 1.0000e-08\nEpoch 91/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0509 - 14x14_loss: 6.0729e-04 - 7x7_loss: 0.0071 - val_loss: 0.1191 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 1.0000e-08\nEpoch 92/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0507 - 14x14_loss: 6.1799e-04 - 7x7_loss: 0.0069 - val_loss: 0.1187 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0744 - lr: 1.0000e-08\nEpoch 93/300\n515/515 [==============================] - 215s 417ms/step - loss: 0.0507 - 14x14_loss: 5.9329e-04 - 7x7_loss: 0.0070 - val_loss: 0.1183 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0740 - lr: 1.0000e-08\nEpoch 94/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0508 - 14x14_loss: 5.8467e-04 - 7x7_loss: 0.0070 - val_loss: 0.1180 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0736 - lr: 1.0000e-08\nEpoch 95/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 5.6044e-04 - 7x7_loss: 0.0072 - val_loss: 0.1187 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0744 - lr: 1.0000e-08\nEpoch 96/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0507 - 14x14_loss: 6.0128e-04 - 7x7_loss: 0.0070 - val_loss: 0.1187 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0744 - lr: 1.0000e-08\nEpoch 97/300\n515/515 [==============================] - 213s 415ms/step - loss: 0.0507 - 14x14_loss: 5.9062e-04 - 7x7_loss: 0.0070 - val_loss: 0.1188 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0744 - lr: 1.0000e-08\nEpoch 98/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0504 - 14x14_loss: 5.6259e-04 - 7x7_loss: 0.0066 - val_loss: 0.1181 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0738 - lr: 1.0000e-08\nEpoch 99/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 6.2326e-04 - 7x7_loss: 0.0071 - val_loss: 0.1196 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0752 - lr: 1.0000e-08\nEpoch 100/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0511 - 14x14_loss: 6.2502e-04 - 7x7_loss: 0.0074 - val_loss: 0.1193 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0749 - lr: 1.0000e-08\nEpoch 101/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 6.0024e-04 - 7x7_loss: 0.0072 - val_loss: 0.1186 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0743 - lr: 1.0000e-08\nEpoch 102/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0506 - 14x14_loss: 5.8144e-04 - 7x7_loss: 0.0069 - val_loss: 0.1197 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0754 - lr: 1.0000e-08\nEpoch 103/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0510 - 14x14_loss: 6.3097e-04 - 7x7_loss: 0.0072 - val_loss: 0.1184 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0741 - lr: 1.0000e-08\nEpoch 104/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0511 - 14x14_loss: 5.8931e-04 - 7x7_loss: 0.0074 - val_loss: 0.1190 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 1.0000e-08\nEpoch 105/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0508 - 14x14_loss: 6.2071e-04 - 7x7_loss: 0.0070 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0746 - lr: 1.0000e-08\nEpoch 106/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0510 - 14x14_loss: 6.0537e-04 - 7x7_loss: 0.0072 - val_loss: 0.1194 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0751 - lr: 1.0000e-08\nEpoch 107/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0506 - 14x14_loss: 5.9820e-04 - 7x7_loss: 0.0069 - val_loss: 0.1193 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0750 - lr: 1.0000e-08\nEpoch 108/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0507 - 14x14_loss: 6.2164e-04 - 7x7_loss: 0.0069 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0746 - lr: 1.0000e-08\nEpoch 109/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0509 - 14x14_loss: 5.8700e-04 - 7x7_loss: 0.0072 - val_loss: 0.1181 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0738 - lr: 1.0000e-08\nEpoch 110/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0510 - 14x14_loss: 5.9296e-04 - 7x7_loss: 0.0073 - val_loss: 0.1189 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0747 - lr: 1.0000e-08\nModel warmed. Loading best val version of model...\nEpoch 1/300\n515/515 [==============================] - 274s 533ms/step - loss: 0.0524 - 14x14_loss: 7.2184e-04 - 7x7_loss: 0.0091 - val_loss: 0.1146 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0712 - lr: 1.0000e-05\nEpoch 2/300\n515/515 [==============================] - 212s 412ms/step - loss: 0.0518 - 14x14_loss: 7.1665e-04 - 7x7_loss: 0.0094 - val_loss: 0.1201 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0776 - lr: 1.0000e-05\nEpoch 3/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0510 - 14x14_loss: 7.2458e-04 - 7x7_loss: 0.0094 - val_loss: 0.1146 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0730 - lr: 1.0000e-05\nEpoch 4/300\n515/515 [==============================] - 274s 531ms/step - loss: 0.0506 - 14x14_loss: 7.2140e-04 - 7x7_loss: 0.0099 - val_loss: 0.1137 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0729 - lr: 1.0000e-05\nEpoch 5/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0501 - 14x14_loss: 7.1441e-04 - 7x7_loss: 0.0102 - val_loss: 0.1226 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0824 - lr: 1.0000e-05\nEpoch 6/300\n515/515 [==============================] - 274s 531ms/step - loss: 0.0487 - 14x14_loss: 7.3038e-04 - 7x7_loss: 0.0094 - val_loss: 0.1120 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0726 - lr: 1.0000e-05\nEpoch 7/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0484 - 14x14_loss: 7.3901e-04 - 7x7_loss: 0.0099 - val_loss: 0.1127 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0740 - lr: 1.0000e-05\nEpoch 8/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0469 - 14x14_loss: 7.3773e-04 - 7x7_loss: 0.0091 - val_loss: 0.1127 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0748 - lr: 1.0000e-05\nEpoch 9/300\n515/515 [==============================] - 274s 531ms/step - loss: 0.0464 - 14x14_loss: 6.9890e-04 - 7x7_loss: 0.0093 - val_loss: 0.1043 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0671 - lr: 1.0000e-05\nEpoch 10/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0459 - 14x14_loss: 7.2245e-04 - 7x7_loss: 0.0095 - val_loss: 0.1175 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0810 - lr: 1.0000e-05\nEpoch 11/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0453 - 14x14_loss: 7.2608e-04 - 7x7_loss: 0.0097 - val_loss: 0.1119 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0761 - lr: 1.0000e-05\nEpoch 12/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0442 - 14x14_loss: 6.9477e-04 - 7x7_loss: 0.0093 - val_loss: 0.1062 - val_14x14_loss: 0.0013 - val_7x7_loss: 0.0711 - lr: 1.0000e-05\nEpoch 13/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0432 - 14x14_loss: 6.9774e-04 - 7x7_loss: 0.0090 - val_loss: 0.1068 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0724 - lr: 1.0000e-05\nEpoch 14/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0429 - 14x14_loss: 7.2372e-04 - 7x7_loss: 0.0094 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0711 - lr: 1.0000e-05\nEpoch 15/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0412 - 14x14_loss: 6.4213e-04 - 7x7_loss: 0.0082 - val_loss: 0.1050 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0715 - lr: 2.0000e-06\nEpoch 16/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0405 - 14x14_loss: 6.0162e-04 - 7x7_loss: 0.0077 - val_loss: 0.1071 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0739 - lr: 2.0000e-06\nEpoch 17/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0403 - 14x14_loss: 6.1065e-04 - 7x7_loss: 0.0078 - val_loss: 0.1050 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0721 - lr: 2.0000e-06\nEpoch 18/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0398 - 14x14_loss: 5.8318e-04 - 7x7_loss: 0.0075 - val_loss: 0.1059 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 2.0000e-06\nEpoch 19/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0393 - 14x14_loss: 6.2184e-04 - 7x7_loss: 0.0073 - val_loss: 0.1056 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 2.0000e-06\nEpoch 20/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0388 - 14x14_loss: 5.7570e-04 - 7x7_loss: 0.0070 - val_loss: 0.1047 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0723 - lr: 4.0000e-07\nEpoch 21/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0388 - 14x14_loss: 5.8046e-04 - 7x7_loss: 0.0070 - val_loss: 0.1052 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0728 - lr: 4.0000e-07\nEpoch 22/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0389 - 14x14_loss: 5.6266e-04 - 7x7_loss: 0.0072 - val_loss: 0.1045 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0722 - lr: 4.0000e-07\nEpoch 23/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0388 - 14x14_loss: 6.0117e-04 - 7x7_loss: 0.0071 - val_loss: 0.1054 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 4.0000e-07\nEpoch 24/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0389 - 14x14_loss: 5.7669e-04 - 7x7_loss: 0.0073 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 4.0000e-07\nEpoch 25/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0390 - 14x14_loss: 5.9075e-04 - 7x7_loss: 0.0074 - val_loss: 0.1053 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0731 - lr: 8.0000e-08\nEpoch 26/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0380 - 14x14_loss: 5.7389e-04 - 7x7_loss: 0.0064 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 8.0000e-08\nEpoch 27/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0385 - 14x14_loss: 5.7714e-04 - 7x7_loss: 0.0070 - val_loss: 0.1053 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 8.0000e-08\nEpoch 28/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0389 - 14x14_loss: 5.6575e-04 - 7x7_loss: 0.0074 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 8.0000e-08\nEpoch 29/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0387 - 14x14_loss: 5.7957e-04 - 7x7_loss: 0.0072 - val_loss: 0.1046 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0725 - lr: 8.0000e-08\nEpoch 30/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0382 - 14x14_loss: 5.5310e-04 - 7x7_loss: 0.0068 - val_loss: 0.1044 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0723 - lr: 1.6000e-08\nEpoch 31/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0383 - 14x14_loss: 5.8689e-04 - 7x7_loss: 0.0068 - val_loss: 0.1053 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 1.6000e-08\nEpoch 32/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0382 - 14x14_loss: 5.6757e-04 - 7x7_loss: 0.0067 - val_loss: 0.1047 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0726 - lr: 1.6000e-08\nEpoch 33/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0386 - 14x14_loss: 5.7128e-04 - 7x7_loss: 0.0072 - val_loss: 0.1051 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0730 - lr: 1.6000e-08\nEpoch 34/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0385 - 14x14_loss: 5.6789e-04 - 7x7_loss: 0.0070 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0728 - lr: 1.6000e-08\nEpoch 35/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0384 - 14x14_loss: 5.6279e-04 - 7x7_loss: 0.0069 - val_loss: 0.1051 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0730 - lr: 1.0000e-08\nEpoch 36/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0383 - 14x14_loss: 5.8226e-04 - 7x7_loss: 0.0069 - val_loss: 0.1056 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0735 - lr: 1.0000e-08\nEpoch 37/300\n515/515 [==============================] - 274s 532ms/step - loss: 0.0382 - 14x14_loss: 5.8581e-04 - 7x7_loss: 0.0067 - val_loss: 0.1043 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0723 - lr: 1.0000e-08\nEpoch 38/300\n515/515 [==============================] - 212s 413ms/step - loss: 0.0385 - 14x14_loss: 5.8061e-04 - 7x7_loss: 0.0070 - val_loss: 0.1051 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0731 - lr: 1.0000e-08\nEpoch 39/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0381 - 14x14_loss: 5.4811e-04 - 7x7_loss: 0.0067 - val_loss: 0.1049 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0728 - lr: 1.0000e-08\nEpoch 40/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0386 - 14x14_loss: 5.8621e-04 - 7x7_loss: 0.0071 - val_loss: 0.1050 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0729 - lr: 1.0000e-08\nEpoch 41/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0384 - 14x14_loss: 6.0214e-04 - 7x7_loss: 0.0069 - val_loss: 0.1050 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0729 - lr: 1.0000e-08\nEpoch 42/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0383 - 14x14_loss: 5.5968e-04 - 7x7_loss: 0.0068 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 1.0000e-08\nEpoch 43/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0385 - 14x14_loss: 5.6305e-04 - 7x7_loss: 0.0070 - val_loss: 0.1049 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0729 - lr: 1.0000e-08\nEpoch 44/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0384 - 14x14_loss: 5.9245e-04 - 7x7_loss: 0.0069 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0728 - lr: 1.0000e-08\nEpoch 45/300\n515/515 [==============================] - 213s 415ms/step - loss: 0.0381 - 14x14_loss: 5.4680e-04 - 7x7_loss: 0.0067 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0728 - lr: 1.0000e-08\nEpoch 46/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0380 - 14x14_loss: 5.5139e-04 - 7x7_loss: 0.0066 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 1.0000e-08\nEpoch 47/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0387 - 14x14_loss: 5.8423e-04 - 7x7_loss: 0.0072 - val_loss: 0.1053 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 1.0000e-08\nEpoch 48/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0383 - 14x14_loss: 5.7085e-04 - 7x7_loss: 0.0069 - val_loss: 0.1047 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 1.0000e-08\nEpoch 49/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0386 - 14x14_loss: 5.8183e-04 - 7x7_loss: 0.0072 - val_loss: 0.1051 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0731 - lr: 1.0000e-08\nEpoch 50/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0382 - 14x14_loss: 5.6014e-04 - 7x7_loss: 0.0068 - val_loss: 0.1046 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0726 - lr: 1.0000e-08\nEpoch 51/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0383 - 14x14_loss: 5.6451e-04 - 7x7_loss: 0.0068 - val_loss: 0.1045 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0725 - lr: 1.0000e-08\nEpoch 52/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0383 - 14x14_loss: 5.6552e-04 - 7x7_loss: 0.0068 - val_loss: 0.1054 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0734 - lr: 1.0000e-08\nEpoch 53/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0384 - 14x14_loss: 5.6281e-04 - 7x7_loss: 0.0069 - val_loss: 0.1054 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0734 - lr: 1.0000e-08\nEpoch 54/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0384 - 14x14_loss: 6.0737e-04 - 7x7_loss: 0.0070 - val_loss: 0.1052 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 1.0000e-08\nEpoch 55/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0380 - 14x14_loss: 5.6872e-04 - 7x7_loss: 0.0065 - val_loss: 0.1047 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0726 - lr: 1.0000e-08\nEpoch 56/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0383 - 14x14_loss: 5.7229e-04 - 7x7_loss: 0.0068 - val_loss: 0.1049 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0729 - lr: 1.0000e-08\nEpoch 57/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0382 - 14x14_loss: 5.7250e-04 - 7x7_loss: 0.0068 - val_loss: 0.1050 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0730 - lr: 1.0000e-08\nEpoch 58/300\n515/515 [==============================] - 213s 413ms/step - loss: 0.0386 - 14x14_loss: 6.0172e-04 - 7x7_loss: 0.0071 - val_loss: 0.1047 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0726 - lr: 1.0000e-08\nEpoch 59/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0381 - 14x14_loss: 5.6364e-04 - 7x7_loss: 0.0067 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0728 - lr: 1.0000e-08\nEpoch 60/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0383 - 14x14_loss: 5.6542e-04 - 7x7_loss: 0.0068 - val_loss: 0.1047 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 1.0000e-08\nEpoch 61/300\n515/515 [==============================] - 214s 416ms/step - loss: 0.0382 - 14x14_loss: 5.6915e-04 - 7x7_loss: 0.0068 - val_loss: 0.1052 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 1.0000e-08\nEpoch 62/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0383 - 14x14_loss: 5.8710e-04 - 7x7_loss: 0.0069 - val_loss: 0.1053 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0733 - lr: 1.0000e-08\nEpoch 63/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0380 - 14x14_loss: 5.5208e-04 - 7x7_loss: 0.0066 - val_loss: 0.1047 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0727 - lr: 1.0000e-08\nEpoch 64/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0387 - 14x14_loss: 6.0329e-04 - 7x7_loss: 0.0073 - val_loss: 0.1052 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0732 - lr: 1.0000e-08\nEpoch 65/300\n515/515 [==============================] - 213s 415ms/step - loss: 0.0381 - 14x14_loss: 5.5636e-04 - 7x7_loss: 0.0067 - val_loss: 0.1048 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0728 - lr: 1.0000e-08\nEpoch 66/300\n515/515 [==============================] - 213s 414ms/step - loss: 0.0384 - 14x14_loss: 5.6379e-04 - 7x7_loss: 0.0070 - val_loss: 0.1057 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0737 - lr: 1.0000e-08\nEpoch 67/300\n515/515 [==============================] - 214s 415ms/step - loss: 0.0383 - 14x14_loss: 5.8824e-04 - 7x7_loss: 0.0069 - val_loss: 0.1051 - val_14x14_loss: 0.0012 - val_7x7_loss: 0.0731 - lr: 1.0000e-08\n"
    }
   ],
   "source": [
    "tf.get_logger().setLevel(40)\n",
    "print('Warming up the model...')\n",
    "train(model,'xception',warmup=True)\n",
    "\n",
    "# Fine tuning\n",
    "print('Model warmed. Loading best val version of model...')\n",
    "del model\n",
    "load_model_path = load_model_folder/f'pt-{model_context}-best_val_loss.ckpt'\n",
    "model = keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),loss=custom_loss)\n",
    "train(model,'xception',warmup=False)\n",
    "\n",
    "# Final save\n",
    "model.save(os.path.join(save_model_folder, 'ft-{}-final.ckpt'.format(model_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 8s 0us/step\n"
    }
   ],
   "source": [
    "#TODO: import hyperopts. play with hyperopts. I picked Xception cause of the high top-5 score despite small parameter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Image ids of pickle and seq: 10153 , 10153\nAre input arrays same?: False\nAre labels same for key=( 7x7 )?: True\nAre labels same for key=( 14x14 )?: True\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_sequence_pickle = TILPickle(val_pickle, 1, aug_identity, conf.input_shape[:-1], label_encoder, preproc_fn, testmode=True)\n",
    "test_sequence = TILSequence(val_imgs_folder, val_annotations, 1, aug_identity, conf.input_shape[:-1], label_encoder, preproc_fn, testmode=True)\n",
    "\n",
    "# Test to make sure that both dispensers dispense the same data\n",
    "img_idx = 42\n",
    "ids_pickle, x_pickle, y_pickle = test_sequence_pickle[img_idx]\n",
    "ids_seq, dims_seq, x_seq, y_seq = test_sequence[img_idx]\n",
    "\n",
    "print('Image ids of pickle and seq:', ids_pickle[0], ',', ids_seq[0])\n",
    "print('Are input arrays same?:', np.allclose( x_pickle, x_seq )) #of course not, one returns the original_image_dim\n",
    "for dimkey, ylabel_pickle in y_pickle.items():\n",
    "    ylabel_seq = y_seq[dimkey]\n",
    "    print('Are labels same for key=(', dimkey, ')?:', np.allclose( ylabel_pickle, ylabel_seq ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'decode_tensor' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-eaea27195383>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mpred_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg_arr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_tensor\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mpred_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maspect_ratios\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# Post-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decode_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "from scripts.encoder import decode_tensor\n",
    "def nms(detections, iou_thresh=0.):\n",
    "    dets_by_class = {}\n",
    "    final_result = []\n",
    "    for det in detections:\n",
    "        cls = det[1]\n",
    "        if cls not in dets_by_class:\n",
    "            dets_by_class[cls] = []\n",
    "        dets_by_class[cls].append( det )\n",
    "    for _, dets in dets_by_class.items():\n",
    "        candidates = list(dets)\n",
    "        candidates.sort( key=lambda x:x[0], reverse=True )\n",
    "        while len(candidates) > 0:\n",
    "            candidate = candidates.pop(0)\n",
    "            _,_,cx,cy,cw,ch = candidate\n",
    "            copy = list(candidates)\n",
    "            for other in candidates:\n",
    "                # Compute the IoU. If it exceeds thresh, we remove it\n",
    "                _,_,ox,oy,ow,oh = other\n",
    "                if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n",
    "                    copy.remove(other)\n",
    "            candidates = list(copy)\n",
    "            final_result.append(candidate)\n",
    "    return final_result\n",
    "\n",
    "# Run this to visualize\n",
    "from IPython.display import Image, display\n",
    "import PIL\n",
    "from PIL import ImageDraw\n",
    "rank_colors = ['cyan', 'magenta', 'pink']\n",
    "det_threshold=0.\n",
    "top_dets=3\n",
    "\n",
    "start=0\n",
    "end=20\n",
    "for k in range(start,end):\n",
    "    _, img_arr, label_cxywh = test_sequence_pickle[k]\n",
    "    img_arr = img_arr[0]\n",
    "    pil_img = PIL.Image.fromarray( (img_arr * 255.).astype(np.uint8) )\n",
    "    W,H = pil_img.size\n",
    "    pred_dict = model(np.array([img_arr]))\n",
    "    preds = decode_tensor( pred_dict, conf.aspect_ratios )\n",
    "\n",
    "    # Post-processing\n",
    "    preds.sort( key=lambda x:x[0], reverse=True )\n",
    "    preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
    "    preds = preds[:top_dets]\n",
    "    preds = nms(preds, iou_thresh=0.5)\n",
    "\n",
    "    draw_img = pil_img.copy()\n",
    "    draw = ImageDraw.Draw(draw_img)\n",
    "    for i, pred in enumerate(preds):\n",
    "        c,cls,x,y,w,h = pred\n",
    "        bb_x = int(x * W)\n",
    "        bb_y = int(y * H)\n",
    "        bb_w = int(w * W)\n",
    "        bb_h = int(h * H)\n",
    "        left = int(bb_x - bb_w / 2)\n",
    "        top = int(bb_y - bb_h / 2)\n",
    "        right = int(bb_x + bb_w / 2)\n",
    "        bot = int(bb_y + bb_h / 2)\n",
    "        cls_str = conf.cat_list[cls-1]\n",
    "\n",
    "        draw.rectangle(((left, top), (right, bot)), outline=rank_colors[i])\n",
    "        draw.text((bb_x, bb_y), cls_str, fill=rank_colors[i])\n",
    "        draw.text( ( int(left + bb_w*.1), int(top + bb_h*.1) ), '{:.2f}'.format(c), fill=rank_colors[i] )\n",
    "\n",
    "    display(draw_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1474/1474 [07:23<00:00,  3.33it/s]\n"
    }
   ],
   "source": [
    "# Generating detections on the folder of validation images\n",
    "from scripts.encoder import decode_tensor\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "detections = []\n",
    "det_threshold=0.\n",
    "for i in tqdm(range(len(test_sequence))):\n",
    "    ids_seq, dims, input_arr, _ = test_sequence[i]\n",
    "    img_id = int(ids_seq[0])\n",
    "    W,H = dims[0]\n",
    "\n",
    "    # Here, I'm inferencing one-by-one, but you can batch it if you want it faster\n",
    "    pred_dict = model.predict(input_arr)\n",
    "    preds = decode_tensor( pred_dict, conf.aspect_ratios )\n",
    "\n",
    "    # Post-processing\n",
    "    preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
    "    preds.sort( key=lambda x:x[0], reverse=True )\n",
    "    preds = preds[:100] # we only evaluate you on 100 detections per image\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "        c,cat_id,x,y,w,h = pred\n",
    "        left = W * (x - w/2.)\n",
    "        left = round(left,1)\n",
    "        top = H * (y - h/2.)\n",
    "        top = round(top,1)\n",
    "        width = W*w\n",
    "        width = round(width,1)\n",
    "        height = H*h\n",
    "        height = round(height,1)\n",
    "        c = float(c)\n",
    "        cat_id = int(cat_id)\n",
    "        detections.append( {'image_id':img_id, 'category_id':cat_id, 'bbox':[left, top, width, height], 'score':c} )\n",
    "\n",
    "with open('detections-7x7-14x14-top100.json', 'w') as f: json.dump(detections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=4.96s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=3.22s).\nAccumulating evaluation results...\nDONE (t=1.36s).\n Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.247\n Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.284\n Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.267\n Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.237\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.196\n"
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "coco_gt = COCO(val_annotations)\n",
    "coco_dt = coco_gt.loadRes('detections-7x7-14x14-top100.json')\n",
    "cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bittilconda431f6b117ae54518ae316f75f88a7342",
   "display_name": "Python 3.8.3 64-bit ('TIL': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}